{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stanford_CS224N_assignment2.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "onoPWYqVfqB0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Upload datasets from Stanford CS224N assignment2.\n",
        "\n",
        "Note that google.colab.files.upload() would upload datasets onto your memories in Chrome!"
      ]
    },
    {
      "metadata": {
        "id": "DkHMK6byVF6t",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "\n",
        "#uploaded = files.upload()\n",
        "\n",
        "#for fn in uploaded.keys():\n",
        "#    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#        name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sr1x_bnww1co",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Get free GPU~~~"
      ]
    },
    {
      "metadata": {
        "id": "BxM9eoH4fXLR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "34b07d37-d4a2-4ee9-c793-3eb4771aca3b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523719753971,
          "user_tz": -480,
          "elapsed": 691,
          "user": {
            "displayName": "姚坤武",
            "photoUrl": "//lh4.googleusercontent.com/-nv4q5H1gHI4/AAAAAAAAAAI/AAAAAAAAiAs/AzMKglvE7Bk/s50-c-k-no/photo.jpg",
            "userId": "104666213213814627780"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fv_gdzAAxcoU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#If so, you are ready to move on.\n",
        "#Implement PyDrive to access data in google drive\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        " \n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        " \n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mH-jC0mCy4A3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "reference tutorial: https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d\n",
        "\n",
        "http://nali.org/load-google-drive-csv-panda-dataframe-google-colab/\n"
      ]
    },
    {
      "metadata": {
        "id": "H75olrnpSUQH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "##general_utils.py\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_minibatches(data, minibatch_size, shuffle=True):\n",
        "    \"\"\"\n",
        "    Iterates through the provided data one minibatch at at time. You can use this function to\n",
        "    iterate through data in minibatches as follows:\n",
        "\n",
        "        for inputs_minibatch in get_minibatches(inputs, minibatch_size):\n",
        "            ...\n",
        "\n",
        "    Or with multiple data sources:\n",
        "\n",
        "        for inputs_minibatch, labels_minibatch in get_minibatches([inputs, labels], minibatch_size):\n",
        "            ...\n",
        "\n",
        "    Args:\n",
        "        data: there are two possible values:\n",
        "            - a list or numpy array\n",
        "            - a list where each element is either a list or numpy array\n",
        "        minibatch_size: the maximum number of items in a minibatch\n",
        "        shuffle: whether to randomize the order of returned data\n",
        "    Returns:\n",
        "        minibatches: the return value depends on data:\n",
        "            - If data is a list/array it yields the next minibatch of data.\n",
        "            - If data a list of lists/arrays it returns the next minibatch of each element in the\n",
        "              list. This can be used to iterate through multiple data sources\n",
        "              (e.g., features and labels) at the same time.\n",
        "\n",
        "    \"\"\"\n",
        "    list_data = type(data) is list and (type(data[0]) is list or type(data[0]) is np.ndarray)\n",
        "    data_size = len(data[0]) if list_data else len(data)\n",
        "    indices = np.arange(data_size)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
        "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
        "        yield [_minibatch(d, minibatch_indices) for d in data] if list_data \\\n",
        "            else _minibatch(data, minibatch_indices)\n",
        "\n",
        "\n",
        "def _minibatch(data, minibatch_idx):\n",
        "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n",
        "\n",
        "\n",
        "def test_all_close(name, actual, expected):\n",
        "    if actual.shape != expected.shape:\n",
        "        raise ValueError(\"{:} failed, expected output to have shape {:} but has shape {:}\"\n",
        "                         .format(name, expected.shape, actual.shape))\n",
        "    if np.amax(np.fabs(actual - expected)) > 1e-6:\n",
        "        raise ValueError(\"{:} failed, expected {:} but value is {:}\".format(name, expected, actual))\n",
        "    else:\n",
        "        print(name, \"passed!\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QA_3MdKHyQPk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "02463253-84b6-426b-feff-0079d8ddf4bb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523719768033,
          "user_tz": -480,
          "elapsed": 810,
          "user": {
            "displayName": "姚坤武",
            "photoUrl": "//lh4.googleusercontent.com/-nv4q5H1gHI4/AAAAAAAAAAI/AAAAAAAAiAs/AzMKglvE7Bk/s50-c-k-no/photo.jpg",
            "userId": "104666213213814627780"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "##q1_softmax.py\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "#print(os.getcwd())\n",
        "#os.chdir('/Users/KunWuYao/GitHub/NLP - Stanford/assignment2/')\n",
        "#from utils.general_utils import test_all_close\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Compute the softmax function in tensorflow.\n",
        "\n",
        "    You might find the tensorflow functions tf.exp, tf.reduce_max,\n",
        "    tf.reduce_sum, tf.expand_dims useful. (Many solutions are possible, so you may\n",
        "    not need to use all of these functions). Recall also that many common\n",
        "    tensorflow operations are sugared (e.g. x + y does elementwise addition\n",
        "    if x and y are both tensors). Make sure to implement the numerical stability\n",
        "    fixes as in the previous homework!\n",
        "\n",
        "    Args:\n",
        "        x:   tf.Tensor with shape (n_samples, n_features). Note feature vectors are\n",
        "                  represented by row-vectors. (For simplicity, no need to handle 1-d\n",
        "                  input as in the previous homework)\n",
        "    Returns:\n",
        "        out: tf.Tensor with shape (n_sample, n_features). You need to construct this\n",
        "                  tensor in this problem.\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    a = tf.subtract(x, tf.reduce_max(x, 1, keep_dims = True))\n",
        "    e = tf.exp(a)\n",
        "    s = tf.reduce_sum(e,1, keep_dims = True)\n",
        "    \n",
        "    out = e / s\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def cross_entropy_loss(y, yhat):\n",
        "    \"\"\"\n",
        "    Compute the cross entropy loss in tensorflow.\n",
        "    The loss should be summed over the current minibatch.\n",
        "\n",
        "    y is a one-hot tensor of shape (n_samples, n_classes) and yhat is a tensor\n",
        "    of shape (n_samples, n_classes). y should be of dtype tf.int32, and yhat should\n",
        "    be of dtype tf.float32.\n",
        "\n",
        "    The functions tf.to_float, tf.reduce_sum, and tf.log might prove useful. (Many\n",
        "    solutions are possible, so you may not need to use all of these functions).\n",
        "\n",
        "    Note: You are NOT allowed to use the tensorflow built-in cross-entropy\n",
        "                functions.\n",
        "\n",
        "    Args:\n",
        "        y:    tf.Tensor with shape (n_samples, n_classes). One-hot encoded.\n",
        "        yhat: tf.Tensorwith shape (n_sample, n_classes). Each row encodes a\n",
        "                    probability distribution and should sum to 1.\n",
        "    Returns:\n",
        "        out:  tf.Tensor with shape (1,) (Scalar output). You need to construct this\n",
        "                    tensor in the problem.\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    y = tf.to_float(y)\n",
        "    out = -tf.reduce_sum(tf.multiply(y, tf.log(yhat)))\n",
        "    \n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def test_softmax_basic():\n",
        "    \"\"\"\n",
        "    Some simple tests of softmax to get you started.\n",
        "    Warning: these are not exhaustive.\n",
        "    \"\"\"\n",
        "\n",
        "    test1 = softmax(tf.constant(np.array([[1001, 1002], [3, 4]]), dtype=tf.float32))\n",
        "    with tf.Session() as sess:\n",
        "        test1 = sess.run(test1)\n",
        "    test_all_close(\"Softmax test 1\", test1, np.array([[0.26894142, 0.73105858],\n",
        "                                                      [0.26894142, 0.73105858]]))\n",
        "\n",
        "    test2 = softmax(tf.constant(np.array([[-1001, -1002]]), dtype=tf.float32))\n",
        "    with tf.Session() as sess:\n",
        "        test2 = sess.run(test2)\n",
        "    test_all_close(\"Softmax test 2\", test2, np.array([[0.73105858, 0.26894142]]))\n",
        "\n",
        "    print(\"Basic (non-exhaustive) softmax tests pass\\n\")\n",
        "\n",
        "\n",
        "def test_cross_entropy_loss_basic():\n",
        "    \"\"\"\n",
        "    Some simple tests of cross_entropy_loss to get you started.\n",
        "    Warning: these are not exhaustive.\n",
        "    \"\"\"\n",
        "    y = np.array([[0, 1], [1, 0], [1, 0]])\n",
        "    yhat = np.array([[.5, .5], [.5, .5], [.5, .5]])\n",
        "\n",
        "    test1 = cross_entropy_loss(tf.constant(y, dtype=tf.int32), tf.constant(yhat, dtype=tf.float32))\n",
        "    with tf.Session() as sess:\n",
        "        test1 = sess.run(test1)\n",
        "    expected = -3 * np.log(0.5)\n",
        "    test_all_close(\"Cross-entropy test 1\", test1, expected)\n",
        "\n",
        "    print(\"Basic (non-exhaustive) cross-entropy tests pass\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_softmax_basic()\n",
        "    test_cross_entropy_loss_basic()\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Softmax test 1 passed!\n",
            "Softmax test 2 passed!\n",
            "Basic (non-exhaustive) softmax tests pass\n",
            "\n",
            "Cross-entropy test 1 passed!\n",
            "Basic (non-exhaustive) cross-entropy tests pass\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "65zQqappSzDA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "##model.py\n",
        "class Model(object):\n",
        "    \"\"\"Abstracts a Tensorflow graph for a learning task.\n",
        "\n",
        "    We use various Model classes as usual abstractions to encapsulate tensorflow\n",
        "    computational graphs. Each algorithm you will construct in this homework will\n",
        "    inherit from a Model object.\n",
        "    \"\"\"\n",
        "    def add_placeholders(self):\n",
        "        \"\"\"Adds placeholder variables to tensorflow computational graph.\n",
        "\n",
        "        Tensorflow uses placeholder variables to represent locations in a\n",
        "        computational graph where data is inserted.  These placeholders are used as\n",
        "        inputs by the rest of the model building and will be fed data during\n",
        "        training.\n",
        "\n",
        "        See for more information:\n",
        "        https://www.tensorflow.org/versions/r0.7/api_docs/python/io_ops.html#placeholders\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
        "\n",
        "    def create_feed_dict(self, inputs_batch, labels_batch=None):\n",
        "        \"\"\"Creates the feed_dict for one step of training.\n",
        "\n",
        "        A feed_dict takes the form of:\n",
        "        feed_dict = {\n",
        "                <placeholder>: <tensor of values to be passed for placeholder>,\n",
        "                ....\n",
        "        }\n",
        "\n",
        "        If labels_batch is None, then no labels are added to feed_dict.\n",
        "\n",
        "        Hint: The keys for the feed_dict should be a subset of the placeholder\n",
        "                    tensors created in add_placeholders.\n",
        "        Args:\n",
        "            inputs_batch: A batch of input data.\n",
        "            labels_batch: A batch of label data.\n",
        "        Returns:\n",
        "            feed_dict: The feed dictionary mapping from placeholders to values.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
        "\n",
        "    def add_prediction_op(self):\n",
        "        \"\"\"Implements the core of the model that transforms a batch of input data into predictions.\n",
        "\n",
        "        Returns:\n",
        "            pred: A tensor of shape (batch_size, n_classes)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
        "\n",
        "    def add_loss_op(self, pred):\n",
        "        \"\"\"Adds Ops for the loss function to the computational graph.\n",
        "\n",
        "        Args:\n",
        "            pred: A tensor of shape (batch_size, n_classes)\n",
        "        Returns:\n",
        "            loss: A 0-d tensor (scalar) output\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
        "\n",
        "    def add_training_op(self, loss):\n",
        "        \"\"\"Sets up the training Ops.\n",
        "\n",
        "        Creates an optimizer and applies the gradients to all trainable variables.\n",
        "        The Op returned by this function is what must be passed to the\n",
        "        sess.run() to train the model. See\n",
        "\n",
        "        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
        "\n",
        "        for more information.\n",
        "\n",
        "        Args:\n",
        "            loss: Loss tensor (a scalar).\n",
        "        Returns:\n",
        "            train_op: The Op for training.\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
        "\n",
        "    def train_on_batch(self, sess, inputs_batch, labels_batch):\n",
        "        \"\"\"Perform one step of gradient descent on the provided batch of data.\n",
        "\n",
        "        Args:\n",
        "            sess: tf.Session()\n",
        "            input_batch: np.ndarray of shape (n_samples, n_features)\n",
        "            labels_batch: np.ndarray of shape (n_samples, n_classes)\n",
        "        Returns:\n",
        "            loss: loss over the batch (a scalar)\n",
        "        \"\"\"\n",
        "        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch)\n",
        "        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)\n",
        "        return loss\n",
        "\n",
        "    def predict_on_batch(self, sess, inputs_batch):\n",
        "        \"\"\"Make predictions for the provided batch of data\n",
        "\n",
        "        Args:\n",
        "            sess: tf.Session()\n",
        "            input_batch: np.ndarray of shape (n_samples, n_features)\n",
        "        Returns:\n",
        "            predictions: np.ndarray of shape (n_samples, n_classes)\n",
        "        \"\"\"\n",
        "        feed = self.create_feed_dict(inputs_batch)\n",
        "        predictions = sess.run(self.pred, feed_dict=feed)\n",
        "        return predictions\n",
        "\n",
        "    def build(self):\n",
        "        self.add_placeholders()\n",
        "        self.pred = self.add_prediction_op()\n",
        "        self.loss = self.add_loss_op(self.pred)\n",
        "        self.train_op = self.add_training_op(self.loss)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tHTgKu9PSgsT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "outputId": "f73df60f-6f95-4bb5-d517-5223f79420f8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523721581226,
          "user_tz": -480,
          "elapsed": 2443,
          "user": {
            "displayName": "姚坤武",
            "photoUrl": "//lh4.googleusercontent.com/-nv4q5H1gHI4/AAAAAAAAAAI/AAAAAAAAiAs/AzMKglvE7Bk/s50-c-k-no/photo.jpg",
            "userId": "104666213213814627780"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#import time\n",
        "\n",
        "#import numpy as np\n",
        "#import tensorflow as tf\n",
        "\n",
        "#from q1_softmax import softmax\n",
        "#from q1_softmax import cross_entropy_loss\n",
        "#from model import Model\n",
        "#from utils.general_utils import get_minibatches\n",
        "\n",
        "\n",
        "class Config1(object):\n",
        "    \"\"\"Holds model hyperparams and data information.\n",
        "\n",
        "    The config class is used to store various hyperparameters and dataset\n",
        "    information parameters. Model objects are passed a Config() object at\n",
        "    instantiation. They can then call self.config.<hyperparameter_name> to\n",
        "    get the hyperparameter settings.\n",
        "    \"\"\"\n",
        "    n_samples = 1024\n",
        "    n_features = 100\n",
        "    n_classes = 5\n",
        "    batch_size = 64\n",
        "    n_epochs = 50\n",
        "    lr = 1e-4\n",
        "    \n",
        "\n",
        "\n",
        "class SoftmaxModel(Model):\n",
        "    \"\"\"Implements a Softmax classifier with cross-entropy loss.\"\"\"\n",
        "\n",
        "    def add_placeholders(self):\n",
        "        \"\"\"Generates placeholder variables to represent the input tensors.\n",
        "\n",
        "        These placeholders are used as inputs by the rest of the model building\n",
        "        and will be fed data during training.\n",
        "\n",
        "        Adds following nodes to the computational graph\n",
        "\n",
        "        input_placeholder: Input placeholder tensor of shape\n",
        "                                              (batch_size, n_features), type tf.float32\n",
        "        labels_placeholder: Labels placeholder tensor of shape\n",
        "                                              (batch_size, n_classes), type tf.int32\n",
        "\n",
        "        Add these placeholders to self as the instance variables\n",
        "            self.input_placeholder\n",
        "            self.labels_placeholder\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE\n",
        "        n_features = self.config1.n_features\n",
        "        n_classes = self.config1.n_classes\n",
        "        \n",
        "        self.input_placeholder = tf.placeholder(tf.float32, shape = (None, n_features))\n",
        "        self.labels_placeholder = tf.placeholder(tf.int32, shape = (None, n_classes))\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def create_feed_dict(self, inputs_batch, labels_batch=None):\n",
        "        \"\"\"Creates the feed_dict for training the given step.\n",
        "\n",
        "        A feed_dict takes the form of:\n",
        "        feed_dict = {\n",
        "                <placeholder>: <tensor of values to be passed for placeholder>,\n",
        "                ....\n",
        "        }\n",
        "\n",
        "        If label_batch is None, then no labels are added to feed_dict.\n",
        "\n",
        "        Hint: The keys for the feed_dict should be the placeholder\n",
        "                tensors created in add_placeholders.\n",
        "\n",
        "        Args:\n",
        "            inputs_batch: A batch of input data.\n",
        "            labels_batch: A batch of label data.\n",
        "        Returns:\n",
        "            feed_dict: The feed dictionary mapping from placeholders to values.\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE\n",
        "        feed_dict = {\n",
        "            self.input_placeholder: inputs_batch,\n",
        "            self.labels_placeholder: labels_batch\n",
        "        }\n",
        "        ### END YOUR CODE\n",
        "        return feed_dict\n",
        "\n",
        "    def add_prediction_op(self):\n",
        "        \"\"\"Adds the core transformation for this model which transforms a batch of input\n",
        "        data into a batch of predictions. In this case, the transformation is a linear layer plus a\n",
        "        softmax transformation:\n",
        "\n",
        "        yhat = softmax(xW + b)\n",
        "\n",
        "        Hint: The input x will be passed in through self.input_placeholder. Each ROW of\n",
        "              self.input_placeholder is a single example. This is usually best-practice for\n",
        "              tensorflow code.\n",
        "        Hint: Make sure to create tf.Variables as needed.\n",
        "        Hint: For this simple use-case, it's sufficient to initialize both weights W\n",
        "                    and biases b with zeros.\n",
        "\n",
        "        Returns:\n",
        "            pred: A tensor of shape (batch_size, n_classes)\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE\n",
        "        n_features = self.config1.n_features\n",
        "        n_classes = self.config1.n_classes\n",
        "        \n",
        "        W = tf.Variable(tf.zeros([n_features, n_classes]))\n",
        "        b = tf.Variable(tf.zeros([n_classes]))\n",
        "        \n",
        "        z = tf.add(tf.matmul(self.input_placeholder, W), b)\n",
        "        pred = softmax(z)\n",
        "        ### END YOUR CODE\n",
        "        return pred\n",
        "\n",
        "    def add_loss_op(self, pred):\n",
        "        \"\"\"Adds cross_entropy_loss ops to the computational graph.\n",
        "\n",
        "        Hint: Use the cross_entropy_loss function we defined. This should be a very\n",
        "                    short function.\n",
        "        Args:\n",
        "            pred: A tensor of shape (batch_size, n_classes)\n",
        "        Returns:\n",
        "            loss: A 0-d tensor (scalar)\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE\n",
        "        loss = cross_entropy_loss(self.labels_placeholder, pred)\n",
        "        ### END YOUR CODE\n",
        "        return loss\n",
        "\n",
        "    def add_training_op(self, loss):\n",
        "        \"\"\"Sets up the training Ops.\n",
        "\n",
        "        Creates an optimizer and applies the gradients to all trainable variables.\n",
        "        The Op returned by this function is what must be passed to the\n",
        "        `sess.run()` call to cause the model to train. See\n",
        "\n",
        "        https://www.tensorflow.org/api_docs/python/tf/train/Optimizer\n",
        "\n",
        "        for more information. Use the learning rate from self.config.\n",
        "\n",
        "        Hint: Use tf.train.GradientDescentOptimizer to get an optimizer object.\n",
        "                    Calling optimizer.minimize() will return a train_op object.\n",
        "\n",
        "        Args:\n",
        "            loss: Loss tensor, from cross_entropy_loss.\n",
        "        Returns:\n",
        "            train_op: The Op for training.\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE\n",
        "        optimizer = tf.train.GradientDescentOptimizer(self.config1.lr)\n",
        "        train_op = optimizer.minimize(loss)\n",
        "        ### END YOUR CODE\n",
        "        return train_op\n",
        "\n",
        "    def run_epoch(self, sess, inputs, labels):\n",
        "        \"\"\"Runs an epoch of training.\n",
        "\n",
        "        Args:\n",
        "            sess: tf.Session() object\n",
        "            inputs: np.ndarray of shape (n_samples, n_features)\n",
        "            labels: np.ndarray of shape (n_samples, n_classes)\n",
        "        Returns:\n",
        "            average_loss: scalar. Average minibatch loss of model on epoch.\n",
        "        \"\"\"\n",
        "        n_minibatches, total_loss = 0, 0\n",
        "        for input_batch, labels_batch in get_minibatches([inputs, labels], self.config1.batch_size):\n",
        "            n_minibatches += 1\n",
        "            total_loss += self.train_on_batch(sess, input_batch, labels_batch)\n",
        "        return total_loss / n_minibatches\n",
        "\n",
        "    def fit(self, sess, inputs, labels):\n",
        "        \"\"\"Fit model on provided data.\n",
        "\n",
        "        Args:\n",
        "            sess: tf.Session()\n",
        "            inputs: np.ndarray of shape (n_samples, n_features)\n",
        "            labels: np.ndarray of shape (n_samples, n_classes)\n",
        "        Returns:\n",
        "            losses: list of loss per epoch\n",
        "        \"\"\"\n",
        "        losses = []\n",
        "        for epoch in range(self.config1.n_epochs):\n",
        "            start_time = time.time()\n",
        "            average_loss = self.run_epoch(sess, inputs, labels)\n",
        "            duration = time.time() - start_time\n",
        "            print ('Epoch {:}: loss = {:.2f} ({:.3f} sec)'.format(epoch, average_loss, duration))\n",
        "            losses.append(average_loss)\n",
        "        return losses\n",
        "\n",
        "    def __init__(self, config1):\n",
        "        \"\"\"Initializes the model.\n",
        "\n",
        "        Args:\n",
        "            config: A model configuration object of type Config\n",
        "        \"\"\"\n",
        "        self.config1 = config1\n",
        "        self.build()\n",
        "\n",
        "\n",
        "def test_softmax_model():\n",
        "    \"\"\"Train softmax model for a number of steps.\"\"\"\n",
        "    config1 = Config1()\n",
        "\n",
        "    # Generate random data to train the model on\n",
        "    np.random.seed(1234)\n",
        "    inputs = np.random.rand(config1.n_samples, config1.n_features)\n",
        "    labels = np.zeros((config1.n_samples, config1.n_classes), dtype=np.int32)\n",
        "    labels[:, 0] = 1\n",
        "\n",
        "    # Tell TensorFlow that the model will be built into the default Graph.\n",
        "    # (not required but good practice)\n",
        "    with tf.Graph().as_default() as graph:\n",
        "        # Build the model and add the variable initializer op\n",
        "        model = SoftmaxModel(config1)\n",
        "        init_op = tf.global_variables_initializer()\n",
        "    # Finalizing the graph causes tensorflow to raise an exception if you try to modify the graph\n",
        "    # further. This is good practice because it makes explicit the distinction between building and\n",
        "    # running the graph.\n",
        "    graph.finalize()\n",
        "\n",
        "    # Create a session for running ops in the graph\n",
        "    with tf.Session(graph=graph) as sess:\n",
        "        # Run the op to initialize the variables.\n",
        "        sess.run(init_op)\n",
        "        # Fit the model\n",
        "        losses = model.fit(sess, inputs, labels)\n",
        "\n",
        "    # If ops are implemented correctly, the average loss should fall close to zero\n",
        "    # rapidly.\n",
        "    assert losses[-1] < .5\n",
        "    print (\"Basic (non-exhaustive) classifier tests pass\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_softmax_model()\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: loss = 59.18 (0.052 sec)\n",
            "Epoch 1: loss = 20.32 (0.032 sec)\n",
            "Epoch 2: loss = 10.92 (0.032 sec)\n",
            "Epoch 3: loss = 7.30 (0.036 sec)\n",
            "Epoch 4: loss = 5.44 (0.032 sec)\n",
            "Epoch 5: loss = 4.32 (0.035 sec)\n",
            "Epoch 6: loss = 3.58 (0.039 sec)\n",
            "Epoch 7: loss = 3.05 (0.032 sec)\n",
            "Epoch 8: loss = 2.65 (0.030 sec)\n",
            "Epoch 9: loss = 2.35 (0.035 sec)\n",
            "Epoch 10: loss = 2.11 (0.032 sec)\n",
            "Epoch 11: loss = 1.91 (0.029 sec)\n",
            "Epoch 12: loss = 1.75 (0.030 sec)\n",
            "Epoch 13: loss = 1.61 (0.035 sec)\n",
            "Epoch 14: loss = 1.49 (0.030 sec)\n",
            "Epoch 15: loss = 1.39 (0.029 sec)\n",
            "Epoch 16: loss = 1.30 (0.037 sec)\n",
            "Epoch 17: loss = 1.22 (0.032 sec)\n",
            "Epoch 18: loss = 1.15 (0.030 sec)\n",
            "Epoch 19: loss = 1.09 (0.028 sec)\n",
            "Epoch 20: loss = 1.03 (0.036 sec)\n",
            "Epoch 21: loss = 0.98 (0.031 sec)\n",
            "Epoch 22: loss = 0.94 (0.031 sec)\n",
            "Epoch 23: loss = 0.89 (0.028 sec)\n",
            "Epoch 24: loss = 0.86 (0.030 sec)\n",
            "Epoch 25: loss = 0.82 (0.029 sec)\n",
            "Epoch 26: loss = 0.79 (0.033 sec)\n",
            "Epoch 27: loss = 0.76 (0.038 sec)\n",
            "Epoch 28: loss = 0.73 (0.031 sec)\n",
            "Epoch 29: loss = 0.71 (0.031 sec)\n",
            "Epoch 30: loss = 0.68 (0.032 sec)\n",
            "Epoch 31: loss = 0.66 (0.032 sec)\n",
            "Epoch 32: loss = 0.64 (0.030 sec)\n",
            "Epoch 33: loss = 0.62 (0.032 sec)\n",
            "Epoch 34: loss = 0.60 (0.038 sec)\n",
            "Epoch 35: loss = 0.58 (0.031 sec)\n",
            "Epoch 36: loss = 0.57 (0.037 sec)\n",
            "Epoch 37: loss = 0.55 (0.034 sec)\n",
            "Epoch 38: loss = 0.54 (0.032 sec)\n",
            "Epoch 39: loss = 0.52 (0.032 sec)\n",
            "Epoch 40: loss = 0.51 (0.031 sec)\n",
            "Epoch 41: loss = 0.50 (0.036 sec)\n",
            "Epoch 42: loss = 0.48 (0.030 sec)\n",
            "Epoch 43: loss = 0.47 (0.032 sec)\n",
            "Epoch 44: loss = 0.46 (0.039 sec)\n",
            "Epoch 45: loss = 0.45 (0.034 sec)\n",
            "Epoch 46: loss = 0.44 (0.030 sec)\n",
            "Epoch 47: loss = 0.43 (0.030 sec)\n",
            "Epoch 48: loss = 0.42 (0.033 sec)\n",
            "Epoch 49: loss = 0.41 (0.031 sec)\n",
            "Basic (non-exhaustive) classifier tests pass\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XoHSWH0bTI3p",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "4ae44865-9c91-41df-caef-10a4551f9bb7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523719801520,
          "user_tz": -480,
          "elapsed": 822,
          "user": {
            "displayName": "姚坤武",
            "photoUrl": "//lh4.googleusercontent.com/-nv4q5H1gHI4/AAAAAAAAAAI/AAAAAAAAiAs/AzMKglvE7Bk/s50-c-k-no/photo.jpg",
            "userId": "104666213213814627780"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class PartialParse(object):\n",
        "    def __init__(self, sentence):\n",
        "        \"\"\"Initializes this partial parse.\n",
        "\n",
        "        Your code should initialize the following fields:\n",
        "            self.stack: The current stack represented as a list with the top of the stack as the\n",
        "                        last element of the list.\n",
        "            self.buffer: The current buffer represented as a list with the first item on the\n",
        "                         buffer as the first item of the list\n",
        "            self.dependencies: The list of dependencies produced so far. Represented as a list of\n",
        "                    tuples where each tuple is of the form (head, dependent).\n",
        "                    Order for this list doesn't matter.\n",
        "\n",
        "        The root token should be represented with the string \"ROOT\"\n",
        "\n",
        "        Args:\n",
        "            sentence: The sentence to be parsed as a list of words.\n",
        "                      Your code should not modify the sentence.\n",
        "        \"\"\"\n",
        "        # The sentence being parsed is kept for bookkeeping purposes. Do not use it in your code.\n",
        "        self.sentence = sentence\n",
        "\n",
        "        ### YOUR CODE HERE\n",
        "        self.stack = [\"ROOT\"]\n",
        "        self.buffer = [word for word in sentence]\n",
        "        self.dependencies = []\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def parse_step(self, transition):\n",
        "        \"\"\"Performs a single parse step by applying the given transition to this partial parse\n",
        "\n",
        "        Args:\n",
        "            transition: A string that equals \"S\", \"LA\", or \"RA\" representing the shift, left-arc,\n",
        "                        and right-arc transitions. You can assume the provided transition is a legal\n",
        "                        transition.\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE\n",
        "        if transition == 'S':\n",
        "            self.stack.append(self.buffer.pop(0))\n",
        "        elif transition == 'LA':\n",
        "            dependent, head = self.stack[-2:]\n",
        "            #Remove the dependent from the stack, keep the head.\n",
        "            self.stack.pop(-2)\n",
        "            self.dependencies.append((head, dependent))\n",
        "        elif transition == 'RA':\n",
        "            head, dependent = self.stack[-2:]\n",
        "            self.stack.pop(-1)\n",
        "            self.dependencies.append((head, dependent))\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def parse(self, transitions):\n",
        "        \"\"\"Applies the provided transitions to this PartialParse\n",
        "\n",
        "        Args:\n",
        "            transitions: The list of transitions in the order they should be applied\n",
        "        Returns:\n",
        "            dependencies: The list of dependencies produced when parsing the sentence. Represented\n",
        "                          as a list of tuples where each tuple is of the form (head, dependent)\n",
        "        \"\"\"\n",
        "        for transition in transitions:\n",
        "            self.parse_step(transition)\n",
        "        return self.dependencies\n",
        "\n",
        "\n",
        "def minibatch_parse(sentences, model, batch_size):\n",
        "    \"\"\"Parses a list of sentences in minibatches using a model.\n",
        "\n",
        "    Args:\n",
        "        sentences: A list of sentences to be parsed (each sentence is a list of words)\n",
        "        model: The model that makes parsing decisions. It is assumed to have a function\n",
        "               model.predict(partial_parses) that takes in a list of PartialParses as input and\n",
        "               returns a list of transitions predicted for each parse. That is, after calling\n",
        "                   transitions = model.predict(partial_parses)\n",
        "               transitions[i] will be the next transition to apply to partial_parses[i].\n",
        "        batch_size: The number of PartialParses to include in each minibatch\n",
        "    Returns:\n",
        "        dependencies: A list where each element is the dependencies list for a parsed sentence.\n",
        "                      Ordering should be the same as in sentences (i.e., dependencies[i] should\n",
        "                      contain the parse for sentences[i]).\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    #[[sentence (stack, buffer, dependencies)], [sentence (stack, buffer, dependencies)]]\n",
        "    partial_parses = [PartialParse(sentence) for sentence in sentences]\n",
        "    unfinished_parses = partial_parses[:]\n",
        "    \n",
        "    while unfinished_parses:\n",
        "        #Set minibatch size and content\n",
        "        minibatch_parses = unfinished_parses[:batch_size]\n",
        "        transitions = model.predict(minibatch_parses)\n",
        "        \n",
        "        #run through the minibatch items\n",
        "        #if not done / no minibatch_parses deleted, keep running with the while loop!\n",
        "        for parse, transition in zip(minibatch_parses, transitions):\n",
        "            #Parse is a sentence (Class, has self.stack, self.buffer, self.dependencies)\n",
        "            parse.parse_step(transition)\n",
        "            if len(parse.stack) < 2 and len(parse.buffer) < 1:\n",
        "                unfinished_parses.remove(parse)\n",
        "    \n",
        "    dependencies = [p.dependencies for p in partial_parses]\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return dependencies\n",
        "\n",
        "\n",
        "def test_step(name, transition, stack, buf, deps,\n",
        "              ex_stack, ex_buf, ex_deps):\n",
        "    \"\"\"Tests that a single parse step returns the expected output\"\"\"\n",
        "    pp = PartialParse([])\n",
        "    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n",
        "\n",
        "    pp.parse_step(transition)\n",
        "    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer), tuple(sorted(pp.dependencies)))\n",
        "    assert stack == ex_stack, \\\n",
        "        \"{:} test resulted in stack {:}, expected {:}\".format(name, stack, ex_stack)\n",
        "    assert buf == ex_buf, \\\n",
        "        \"{:} test resulted in buffer {:}, expected {:}\".format(name, buf, ex_buf)\n",
        "    assert deps == ex_deps, \\\n",
        "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
        "    print(\"{:} test passed!\".format(name))\n",
        "\n",
        "\n",
        "def test_parse_step():\n",
        "    \"\"\"Simple tests for the PartialParse.parse_step function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "    test_step(\"SHIFT\", \"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [],\n",
        "              (\"ROOT\", \"the\", \"cat\"), (\"sat\",), ())\n",
        "    test_step(\"LEFT-ARC\", \"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [],\n",
        "              (\"ROOT\", \"cat\",), (\"sat\",), ((\"cat\", \"the\"),))\n",
        "    test_step(\"RIGHT-ARC\", \"RA\", [\"ROOT\", \"run\", \"fast\"], [], [],\n",
        "              (\"ROOT\", \"run\",), (), ((\"run\", \"fast\"),))\n",
        "\n",
        "\n",
        "def test_parse():\n",
        "    \"\"\"Simple tests for the PartialParse.parse function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "    sentence = [\"parse\", \"this\", \"sentence\"]\n",
        "    dependencies = PartialParse(sentence).parse([\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n",
        "    dependencies = tuple(sorted(dependencies))\n",
        "    expected = (('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))\n",
        "    assert dependencies == expected,  \\\n",
        "        \"parse test resulted in dependencies {:}, expected {:}\".format(dependencies, expected)\n",
        "    assert tuple(sentence) == (\"parse\", \"this\", \"sentence\"), \\\n",
        "        \"parse test failed: the input sentence should not be modified\"\n",
        "    print(\"parse test passed!\")\n",
        "\n",
        "\n",
        "class DummyModel(object):\n",
        "    \"\"\"Dummy model for testing the minibatch_parse function\n",
        "    First shifts everything onto the stack and then does exclusively right arcs if the first word of\n",
        "    the sentence is \"right\", \"left\" if otherwise.\n",
        "    \"\"\"\n",
        "    def predict(self, partial_parses):\n",
        "        return [(\"RA\" if pp.stack[1] is \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
        "                for pp in partial_parses]\n",
        "\n",
        "\n",
        "def test_dependencies(name, deps, ex_deps):\n",
        "    \"\"\"Tests the provided dependencies match the expected dependencies\"\"\"\n",
        "    deps = tuple(sorted(deps))\n",
        "    assert deps == ex_deps, \\\n",
        "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
        "\n",
        "\n",
        "def test_minibatch_parse():\n",
        "    \"\"\"Simple tests for the minibatch_parse function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "    sentences = [[\"right\", \"arcs\", \"only\"],\n",
        "                 [\"right\", \"arcs\", \"only\", \"again\"],\n",
        "                 [\"left\", \"arcs\", \"only\"],\n",
        "                 [\"left\", \"arcs\", \"only\", \"again\"]]\n",
        "    deps = minibatch_parse(sentences, DummyModel(), 2)\n",
        "    test_dependencies(\"minibatch_parse\", deps[0],\n",
        "                      (('ROOT', 'right'), ('arcs', 'only'), ('right', 'arcs')))\n",
        "    test_dependencies(\"minibatch_parse\", deps[1],\n",
        "                      (('ROOT', 'right'), ('arcs', 'only'), ('only', 'again'), ('right', 'arcs')))\n",
        "    test_dependencies(\"minibatch_parse\", deps[2],\n",
        "                      (('only', 'ROOT'), ('only', 'arcs'), ('only', 'left')))\n",
        "    test_dependencies(\"minibatch_parse\", deps[3],\n",
        "                      (('again', 'ROOT'), ('again', 'arcs'), ('again', 'left'), ('again', 'only')))\n",
        "    print(\"minibatch_parse test passed!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    test_parse_step()\n",
        "    test_parse()\n",
        "    test_minibatch_parse()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SHIFT test passed!\n",
            "LEFT-ARC test passed!\n",
            "RIGHT-ARC test passed!\n",
            "parse test passed!\n",
            "minibatch_parse test passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zm9SNYPITPKC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "963e2a56-34e2-4645-fa18-57ae5bb0736c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523719807814,
          "user_tz": -480,
          "elapsed": 640,
          "user": {
            "displayName": "姚坤武",
            "photoUrl": "//lh4.googleusercontent.com/-nv4q5H1gHI4/AAAAAAAAAAI/AAAAAAAAiAs/AzMKglvE7Bk/s50-c-k-no/photo.jpg",
            "userId": "104666213213814627780"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#import numpy as np\n",
        "#import tensorflow as tf\n",
        "\n",
        "\n",
        "def xavier_weight_init():\n",
        "    \"\"\"Returns function that creates random tensor.\n",
        "\n",
        "    The specified function will take in a shape (tuple or 1-d array) and\n",
        "    returns a random tensor of the specified shape drawn from the\n",
        "    Xavier initialization distribution.\n",
        "\n",
        "    Hint: You might find tf.random_uniform useful.\n",
        "    \"\"\"\n",
        "    def _xavier_initializer(shape, **kwargs):\n",
        "        \"\"\"Defines an initializer for the Xavier distribution.\n",
        "        Specifically, the output should be sampled uniformly from [-epsilon, epsilon] where\n",
        "            epsilon = sqrt(6) / <sum of the sizes of shape's dimensions>\n",
        "        e.g., if shape = (2, 3), epsilon = sqrt(6 / (2 + 3))\n",
        "\n",
        "        This function will be used as a variable initializer.\n",
        "\n",
        "        Args:\n",
        "            shape: Tuple or 1-d array that species the dimensions of the requested tensor.\n",
        "        Returns:\n",
        "            out: tf.Tensor of specified shape sampled from the Xavier distribution.\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE\n",
        "        epsilon = tf.sqrt(6.0 / tf.cast(tf.reduce_sum(shape), tf.float32))\n",
        "        out = tf.random_uniform(shape, -epsilon, epsilon)\n",
        "        ### END YOUR CODE\n",
        "        return out\n",
        "    # Returns defined initializer function.\n",
        "    return _xavier_initializer\n",
        "\n",
        "\n",
        "def test_initialization_basic():\n",
        "    \"\"\"Some simple tests for the initialization.\n",
        "    \"\"\"\n",
        "    print (\"Running basic tests...\")\n",
        "    xavier_initializer = xavier_weight_init()\n",
        "    shape = (1,)\n",
        "    xavier_mat = xavier_initializer(shape)\n",
        "    assert xavier_mat.get_shape() == shape\n",
        "\n",
        "    shape = (1, 2, 3)\n",
        "    xavier_mat = xavier_initializer(shape)\n",
        "    assert xavier_mat.get_shape() == shape\n",
        "    print (\"Basic (non-exhaustive) Xavier initialization tests pass\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_initialization_basic()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running basic tests...\n",
            "Basic (non-exhaustive) Xavier initialization tests pass\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kOQ3pEdHcl75",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How to get file id/location in the GoogleDrive object!\n",
        "\n",
        "https://stackoverflow.com/questions/48376580/google-colab-how-to-read-data-from-my-google-drive"
      ]
    },
    {
      "metadata": {
        "id": "48-Y7EScU4z2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "05222dc4-7e90-441e-8619-dd78e649bc5e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523723665408,
          "user_tz": -480,
          "elapsed": 701,
          "user": {
            "displayName": "姚坤武",
            "photoUrl": "//lh4.googleusercontent.com/-nv4q5H1gHI4/AAAAAAAAAAI/AAAAAAAAiAs/AzMKglvE7Bk/s50-c-k-no/photo.jpg",
            "userId": "104666213213814627780"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "file_list = drive.ListFile({'q': \"'1lG4pz5tkM8qvuHs5tEfB0Ew7dZKnddTJ' in parents\", 'maxResults': 50}).GetList()\n",
        "print('Received %s files from Files.list()' % len(file_list)) # <= 10\n",
        "for file1 in file_list:\n",
        "    print('title: %s, id: %s' % (file1['title'], file1['id']))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Received 9 files from Files.list()\n",
            "title: dev.conll, id: 1qQdaVA7vltM7k_mAEA7BfosExu7jEqR2\n",
            "title: weight, id: 1CrxPRgbU-lQamwg--9FuuJWrDd5plC0z\n",
            "title: weight, id: 16zfXFh4xDR_GB1K_X_aG_wJMfsFK5v8T\n",
            "title: train.gold.conll, id: 1DAh9a83MRgqv95eaj3LTUZA4KLK2zlY_\n",
            "title: train.conll, id: 155yq13fVwHypPVicbZdtOigSu8sQt19z\n",
            "title: test.gold.conll, id: 1eXvxO5y52kZuE5P8Ruz8RuFKKVjD0aHi\n",
            "title: test.conll, id: 1Dgy3UZNyB_beX6JSh_5HrkjaTD_R_ewc\n",
            "title: en-cw.txt, id: 1stVKwKmn6lO7LQz8jPqcwBaRwCEyTV4-\n",
            "title: dev.gold.conll, id: 119gX_LgivBhTcfkbBXpnNIJ0OUauq4j7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8e6tcVxCf-U4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load Google Drive CSV into Pandas DataFrame for Google Colaboratory\n",
        "\n",
        "http://nali.org/load-google-drive-csv-panda-dataframe-google-colab/"
      ]
    },
    {
      "metadata": {
        "id": "xed1qaFEfbmX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#embedding_file = drive.CreateFile({'id': '1stVKwKmn6lO7LQz8jPqcwBaRwCEyTV4-'})\n",
        "#embedding_file.GetContentFile('en-cw.txt')\n",
        "\n",
        "#for line in embedding_file:\n",
        "#    print(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0-3M-jlI0QTY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#for line in open('en-cw.txt').readlines():\n",
        "#    print(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xLg8NwjNTsnx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "##parser_utils.py\n",
        "\"\"\"Utilities for training the dependency parser.\n",
        "You do not need to read/understand this code\n",
        "\"\"\"\n",
        "\n",
        "#import time\n",
        "#import os\n",
        "import logging\n",
        "from collections import Counter\n",
        "#from utils.general_utils import get_minibatches\n",
        "#from q2_parser_transitions import minibatch_parse\n",
        "\n",
        "#import numpy as np\n",
        "\n",
        "\n",
        "P_PREFIX = '<p>:'\n",
        "L_PREFIX = '<l>:'\n",
        "UNK = '<UNK>'\n",
        "NULL = '<NULL>'\n",
        "ROOT = '<ROOT>'\n",
        "\n",
        "\n",
        "class Config2(object):\n",
        "    language = 'english'\n",
        "    with_punct = True\n",
        "    unlabeled = True\n",
        "    lowercase = True\n",
        "    use_pos = True\n",
        "    use_dep = True\n",
        "    use_dep = use_dep and (not unlabeled)\n",
        "    data_path = './data'\n",
        "    train_file = drive.CreateFile({'id': '155yq13fVwHypPVicbZdtOigSu8sQt19z'})\n",
        "    train_file.GetContentFile('train.conll')\n",
        "    dev_file = drive.CreateFile({'id': '1qQdaVA7vltM7k_mAEA7BfosExu7jEqR2'})\n",
        "    dev_file.GetContentFile('dev.conll')\n",
        "    test_file = drive.CreateFile({'id': '1Dgy3UZNyB_beX6JSh_5HrkjaTD_R_ewc'})\n",
        "    test_file.GetContentFile('test.conll')\n",
        "    embedding_file = drive.CreateFile({'id': '1stVKwKmn6lO7LQz8jPqcwBaRwCEyTV4-'})\n",
        "    embedding_file.GetContentFile('en-cw.txt')\n",
        "\n",
        "\n",
        "class Parser(object):\n",
        "    \"\"\"Contains everything needed for transition-based dependency parsing except for the model\"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        root_labels = list([l for ex in dataset\n",
        "                           for (h, l) in zip(ex['head'], ex['label']) if h == 0])\n",
        "        counter = Counter(root_labels)\n",
        "        if len(counter) > 1:\n",
        "            logging.info('Warning: more than one root label')\n",
        "            logging.info(counter)\n",
        "        self.root_label = counter.most_common()[0][0]\n",
        "        deprel = [self.root_label] + list(set([w for ex in dataset\n",
        "                                               for w in ex['label']\n",
        "                                               if w != self.root_label]))\n",
        "        tok2id = {L_PREFIX + l: i for (i, l) in enumerate(deprel)}\n",
        "        tok2id[L_PREFIX + NULL] = self.L_NULL = len(tok2id)\n",
        "\n",
        "        config2 = Config2()\n",
        "        self.unlabeled = config2.unlabeled\n",
        "        self.with_punct = config2.with_punct\n",
        "        self.use_pos = config2.use_pos\n",
        "        self.use_dep = config2.use_dep\n",
        "        self.language = config2.language\n",
        "\n",
        "        if self.unlabeled:\n",
        "            trans = ['L', 'R', 'S']\n",
        "            self.n_deprel = 1\n",
        "        else:\n",
        "            trans = ['L-' + l for l in deprel] + ['R-' + l for l in deprel] + ['S']\n",
        "            self.n_deprel = len(deprel)\n",
        "\n",
        "        self.n_trans = len(trans)\n",
        "        self.tran2id = {t: i for (i, t) in enumerate(trans)}\n",
        "        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n",
        "\n",
        "        # logging.info('Build dictionary for part-of-speech tags.')\n",
        "        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n",
        "                                  offset=len(tok2id)))\n",
        "        tok2id[P_PREFIX + UNK] = self.P_UNK = len(tok2id)\n",
        "        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
        "        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
        "\n",
        "        # logging.info('Build dictionary for words.')\n",
        "        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n",
        "                                  offset=len(tok2id)))\n",
        "        tok2id[UNK] = self.UNK = len(tok2id)\n",
        "        tok2id[NULL] = self.NULL = len(tok2id)\n",
        "        tok2id[ROOT] = self.ROOT = len(tok2id)\n",
        "\n",
        "        self.tok2id = tok2id\n",
        "        self.id2tok = {v: k for (k, v) in tok2id.items()}\n",
        "\n",
        "        self.n_features = 18 + (18 if config2.use_pos else 0) + (12 if config2.use_dep else 0)\n",
        "        self.n_tokens = len(tok2id)\n",
        "\n",
        "    def vectorize(self, examples):\n",
        "        vec_examples = []\n",
        "        for ex in examples:\n",
        "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
        "                                  else self.UNK for w in ex['word']]\n",
        "            pos = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
        "                                   else self.P_UNK for w in ex['pos']]\n",
        "            head = [-1] + ex['head']\n",
        "            label = [-1] + [self.tok2id[L_PREFIX + w] if L_PREFIX + w in self.tok2id\n",
        "                            else -1 for w in ex['label']]\n",
        "            vec_examples.append({'word': word, 'pos': pos,\n",
        "                                 'head': head, 'label': label})\n",
        "        return vec_examples\n",
        "\n",
        "    def extract_features(self, stack, buf, arcs, ex):\n",
        "        if stack[0] == \"ROOT\":\n",
        "            stack[0] = 0\n",
        "\n",
        "        def get_lc(k):\n",
        "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n",
        "\n",
        "        def get_rc(k):\n",
        "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n",
        "                          reverse=True)\n",
        "\n",
        "        p_features = []\n",
        "        l_features = []\n",
        "        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n",
        "        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n",
        "        if self.use_pos:\n",
        "            p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n",
        "            p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n",
        "\n",
        "        for i in range(2):\n",
        "            if i < len(stack):\n",
        "                k = stack[-i-1]\n",
        "                lc = get_lc(k)\n",
        "                rc = get_rc(k)\n",
        "                llc = get_lc(lc[0]) if len(lc) > 0 else []\n",
        "                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n",
        "\n",
        "                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n",
        "                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n",
        "                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n",
        "\n",
        "                if self.use_pos:\n",
        "                    p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n",
        "\n",
        "                if self.use_dep:\n",
        "                    l_features.append(ex['label'][lc[0]] if len(lc) > 0 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][rc[0]] if len(rc) > 0 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][lc[1]] if len(lc) > 1 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][rc[1]] if len(rc) > 1 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][llc[0]] if len(llc) > 0 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][rrc[0]] if len(rrc) > 0 else self.L_NULL)\n",
        "            else:\n",
        "                features += [self.NULL] * 6\n",
        "                if self.use_pos:\n",
        "                    p_features += [self.P_NULL] * 6\n",
        "                if self.use_dep:\n",
        "                    l_features += [self.L_NULL] * 6\n",
        "\n",
        "        features += p_features + l_features\n",
        "        assert len(features) == self.n_features\n",
        "        return features\n",
        "\n",
        "    def get_oracle(self, stack, buf, ex):\n",
        "        if len(stack) < 2:\n",
        "            return self.n_trans - 1\n",
        "\n",
        "        i0 = stack[-1]\n",
        "        i1 = stack[-2]\n",
        "        h0 = ex['head'][i0]\n",
        "        h1 = ex['head'][i1]\n",
        "        l0 = ex['label'][i0]\n",
        "        l1 = ex['label'][i1]\n",
        "\n",
        "        if self.unlabeled:\n",
        "            if (i1 > 0) and (h1 == i0):\n",
        "                return 0\n",
        "            elif (i1 >= 0) and (h0 == i1) and \\\n",
        "                 (not any([x for x in buf if ex['head'][x] == i0])):\n",
        "                return 1\n",
        "            else:\n",
        "                return None if len(buf) == 0 else 2\n",
        "        else:\n",
        "            if (i1 > 0) and (h1 == i0):\n",
        "                return l1 if (l1 >= 0) and (l1 < self.n_deprel) else None\n",
        "            elif (i1 >= 0) and (h0 == i1) and \\\n",
        "                 (not any([x for x in buf if ex['head'][x] == i0])):\n",
        "                return l0 + self.n_deprel if (l0 >= 0) and (l0 < self.n_deprel) else None\n",
        "            else:\n",
        "                return None if len(buf) == 0 else self.n_trans - 1\n",
        "\n",
        "    def create_instances(self, examples):\n",
        "        all_instances = []\n",
        "        succ = 0\n",
        "        for id, ex in enumerate(examples):\n",
        "            n_words = len(ex['word']) - 1\n",
        "\n",
        "            # arcs = {(h, t, label)}\n",
        "            stack = [0]\n",
        "            buf = [i + 1 for i in range(n_words)]\n",
        "            arcs = []\n",
        "            instances = []\n",
        "            for i in range(n_words * 2):\n",
        "                gold_t = self.get_oracle(stack, buf, ex)\n",
        "                if gold_t is None:\n",
        "                    break\n",
        "                legal_labels = self.legal_labels(stack, buf)\n",
        "                assert legal_labels[gold_t] == 1\n",
        "                instances.append((self.extract_features(stack, buf, arcs, ex),\n",
        "                                  legal_labels, gold_t))\n",
        "                if gold_t == self.n_trans - 1:\n",
        "                    stack.append(buf[0])\n",
        "                    buf = buf[1:]\n",
        "                elif gold_t < self.n_deprel:\n",
        "                    arcs.append((stack[-1], stack[-2], gold_t))\n",
        "                    stack = stack[:-2] + [stack[-1]]\n",
        "                else:\n",
        "                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n",
        "                    stack = stack[:-1]\n",
        "            else:\n",
        "                succ += 1\n",
        "                all_instances += instances\n",
        "\n",
        "        return all_instances\n",
        "\n",
        "    def legal_labels(self, stack, buf):\n",
        "        labels = ([1] if len(stack) > 2 else [0]) * self.n_deprel\n",
        "        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel\n",
        "        labels += [1] if len(buf) > 0 else [0]\n",
        "        return labels\n",
        "\n",
        "    def parse(self, dataset, eval_batch_size=5000):\n",
        "        sentences = []\n",
        "        sentence_id_to_idx = {}\n",
        "        for i, example in enumerate(dataset):\n",
        "            n_words = len(example['word']) - 1\n",
        "            sentence = [j + 1 for j in range(n_words)]\n",
        "            sentences.append(sentence)\n",
        "            sentence_id_to_idx[id(sentence)] = i\n",
        "\n",
        "        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n",
        "        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n",
        "\n",
        "        UAS = all_tokens = 0.0\n",
        "        for i, ex in enumerate(dataset):\n",
        "            head = [-1] * len(ex['word'])\n",
        "            for h, t, in dependencies[i]:\n",
        "                head[t] = h\n",
        "            for pred_h, gold_h, gold_l, pos in \\\n",
        "                    zip(head[1:], ex['head'][1:], ex['label'][1:], ex['pos'][1:]):\n",
        "                    assert self.id2tok[pos].startswith(P_PREFIX)\n",
        "                    pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
        "                    if (self.with_punct) or (not punct(self.language, pos_str)):\n",
        "                        UAS += 1 if pred_h == gold_h else 0\n",
        "                        all_tokens += 1\n",
        "        UAS /= all_tokens\n",
        "        return UAS, dependencies\n",
        "\n",
        "\n",
        "class ModelWrapper(object):\n",
        "    def __init__(self, parser, dataset, sentence_id_to_idx):\n",
        "        self.parser = parser\n",
        "        self.dataset = dataset\n",
        "        self.sentence_id_to_idx = sentence_id_to_idx\n",
        "\n",
        "    def predict(self, partial_parses):\n",
        "        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dependencies,\n",
        "                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n",
        "                for p in partial_parses]\n",
        "        mb_x = np.array(mb_x).astype('int32')\n",
        "        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n",
        "        pred = self.parser.model.predict_on_batch(self.parser.session, mb_x)\n",
        "        pred = np.argmax(pred + 10000 * np.array(mb_l).astype('float32'), 1)\n",
        "        pred = [\"S\" if p == 2 else (\"LA\" if p == 0 else \"RA\") for p in pred]\n",
        "        return pred\n",
        "\n",
        "\n",
        "def read_conll(in_file, lowercase=False, max_example=None):\n",
        "    examples = []\n",
        "    with open(in_file) as f:\n",
        "        word, pos, head, label = [], [], [], []\n",
        "        for line in f.readlines():\n",
        "            sp = line.strip().split('\\t')\n",
        "            if len(sp) == 10:\n",
        "                if '-' not in sp[0]:\n",
        "                    word.append(sp[1].lower() if lowercase else sp[1])\n",
        "                    pos.append(sp[4])\n",
        "                    head.append(int(sp[6]))\n",
        "                    label.append(sp[7])\n",
        "            elif len(word) > 0:\n",
        "                examples.append({'word': word, 'pos': pos, 'head': head, 'label': label})\n",
        "                word, pos, head, label = [], [], [], []\n",
        "                if (max_example is not None) and (len(examples) == max_example):\n",
        "                    break\n",
        "        if len(word) > 0:\n",
        "            examples.append({'word': word, 'pos': pos, 'head': head, 'label': label})\n",
        "    return examples\n",
        "\n",
        "\n",
        "def build_dict(keys, n_max=None, offset=0):\n",
        "    count = Counter()\n",
        "    for key in keys:\n",
        "        count[key] += 1\n",
        "    ls = count.most_common() if n_max is None \\\n",
        "        else count.most_common(n_max)\n",
        "\n",
        "    return {w[0]: index + offset for (index, w) in enumerate(ls)}\n",
        "\n",
        "\n",
        "def punct(language, pos):\n",
        "    if language == 'english':\n",
        "        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n",
        "    elif language == 'chinese':\n",
        "        return pos == 'PU'\n",
        "    elif language == 'french':\n",
        "        return pos == 'PUNC'\n",
        "    elif language == 'german':\n",
        "        return pos in [\"$.\", \"$,\", \"$[\"]\n",
        "    elif language == 'spanish':\n",
        "        # http://nlp.stanford.edu/software/spanish-faq.shtml\n",
        "        return pos in [\"f0\", \"faa\", \"fat\", \"fc\", \"fd\", \"fe\", \"fg\", \"fh\",\n",
        "                       \"fia\", \"fit\", \"fp\", \"fpa\", \"fpt\", \"fs\", \"ft\",\n",
        "                       \"fx\", \"fz\"]\n",
        "    elif language == 'universal':\n",
        "        return pos == 'PUNCT'\n",
        "    else:\n",
        "        raise ValueError('language: %s is not supported.' % language)\n",
        "\n",
        "\n",
        "def minibatches(data, batch_size):\n",
        "    x = np.array([d[0] for d in data])\n",
        "    y = np.array([d[2] for d in data])\n",
        "    one_hot = np.zeros((y.size, 3))\n",
        "    one_hot[np.arange(y.size), y] = 1\n",
        "    return get_minibatches([x, one_hot], batch_size)\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(reduced=True):\n",
        "    config2 = Config2()\n",
        "\n",
        "    print (\"Loading data...\",)\n",
        "    start = time.time()\n",
        "    train_set = read_conll('train.conll',\n",
        "                           lowercase=config2.lowercase)\n",
        "    dev_set = read_conll('dev.conll',\n",
        "                         lowercase=config2.lowercase)\n",
        "    test_set = read_conll('test.conll',\n",
        "                          lowercase=config2.lowercase)\n",
        "    if reduced:\n",
        "        train_set = train_set[:1000]\n",
        "        dev_set = dev_set[:500]\n",
        "        test_set = test_set[:500]\n",
        "    print( \"took {:.2f} seconds\".format(time.time() - start))\n",
        "\n",
        "    print( \"Building parser...\",)\n",
        "    start = time.time()\n",
        "    parser = Parser(train_set)\n",
        "    print( \"took {:.2f} seconds\".format(time.time() - start))\n",
        "\n",
        "    print( \"Loading pretrained embeddings...\",)\n",
        "    start = time.time()\n",
        "    word_vectors = {}\n",
        "    for line in open('en-cw.txt').readlines():\n",
        "        sp = line.strip().split()\n",
        "        word_vectors[sp[0]] = [float(x) for x in sp[1:]]\n",
        "    embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
        "\n",
        "    for token in parser.tok2id:\n",
        "        i = parser.tok2id[token]\n",
        "        if token in word_vectors:\n",
        "            embeddings_matrix[i] = word_vectors[token]\n",
        "        elif token.lower() in word_vectors:\n",
        "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
        "    print( \"took {:.2f} seconds\".format(time.time() - start))\n",
        "\n",
        "    print( \"Vectorizing data...\",)\n",
        "    start = time.time()\n",
        "    train_set = parser.vectorize(train_set)\n",
        "    dev_set = parser.vectorize(dev_set)\n",
        "    test_set = parser.vectorize(test_set)\n",
        "    print( \"took {:.2f} seconds\".format(time.time() - start))\n",
        "\n",
        "    print( \"Preprocessing training data...\",)\n",
        "    start = time.time()\n",
        "    train_examples = parser.create_instances(train_set)\n",
        "    print( \"took {:.2f} seconds\".format(time.time() - start))\n",
        "\n",
        "    return parser, embeddings_matrix, train_examples, dev_set, test_set,\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pass\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "raIrd-fITgG-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1687
        },
        "outputId": "e2f67cd8-3a94-4344-a3a1-1af268a82658",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523726460343,
          "user_tz": -480,
          "elapsed": 420623,
          "user": {
            "displayName": "姚坤武",
            "photoUrl": "//lh4.googleusercontent.com/-nv4q5H1gHI4/AAAAAAAAAAI/AAAAAAAAiAs/AzMKglvE7Bk/s50-c-k-no/photo.jpg",
            "userId": "104666213213814627780"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle as cPickle\n",
        "#import os\n",
        "#import time\n",
        "#import tensorflow as tf\n",
        "\n",
        "#from model import Model\n",
        "#from q2_initialization import xavier_weight_init\n",
        "#from utils.parser_utils import minibatches, load_and_preprocess_data\n",
        "\n",
        "\n",
        "class Config3(object):\n",
        "    \"\"\"Holds model hyperparams and data information.\n",
        "\n",
        "    The config class is used to store various hyperparameters and dataset\n",
        "    information parameters. Model objects are passed a Config() object at\n",
        "    instantiation. They can then call self.config.<hyperparameter_name> to\n",
        "    get the hyperparameter settings.\n",
        "    \"\"\"\n",
        "    n_features = 36\n",
        "    n_classes = 3\n",
        "    dropout = 0.5  # (p_drop in the handout)\n",
        "    embed_size = 50\n",
        "    hidden_size = 200\n",
        "    batch_size = 1024\n",
        "    n_epochs = 15\n",
        "    lr = 0.0005\n",
        "\n",
        "\n",
        "class ParserModel(Model):\n",
        "    \"\"\"\n",
        "    Implements a feedforward neural network with an embedding layer and single hidden layer.\n",
        "    This network will predict which transition should be applied to a given partial parse\n",
        "    configuration.\n",
        "    \"\"\"\n",
        "\n",
        "    def add_placeholders(self):\n",
        "        \"\"\"Generates placeholder variables to represent the input tensors\n",
        "\n",
        "        These placeholders are used as inputs by the rest of the model building and will be fed\n",
        "        data during training.  Note that when \"None\" is in a placeholder's shape, it's flexible\n",
        "        (so we can use different batch sizes without rebuilding the model).\n",
        "\n",
        "        Adds following nodes to the computational graph\n",
        "\n",
        "        input_placeholder: Input placeholder tensor of  shape (None, n_features), type tf.int32\n",
        "        labels_placeholder: Labels placeholder tensor of shape (None, n_classes), type tf.float32\n",
        "        dropout_placeholder: Dropout value placeholder (scalar), type tf.float32\n",
        "\n",
        "        Add these placeholders to self as the instance variables\n",
        "            self.input_placeholder\n",
        "            self.labels_placeholder\n",
        "            self.dropout_placeholder\n",
        "\n",
        "        (Don't change the variable names)\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE\n",
        "        n_features = self.config3.n_features\n",
        "        n_classes = self.config3.n_classes\n",
        "        \n",
        "        self.input_placeholder = tf.placeholder(tf.int32, shape = (None, n_features))\n",
        "        self.labels_placeholder = tf.placeholder(tf.float32, shape = (None, n_classes))\n",
        "        self.dropout_placeholder = tf.placeholder(tf.float32)\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def create_feed_dict(self, inputs_batch, labels_batch=None, dropout=0):\n",
        "        \"\"\"Creates the feed_dict for the dependency parser.\n",
        "\n",
        "        A feed_dict takes the form of:\n",
        "\n",
        "        feed_dict = {\n",
        "                <placeholder>: <tensor of values to be passed for placeholder>,\n",
        "                ....\n",
        "        }\n",
        "\n",
        "\n",
        "        Hint: The keys for the feed_dict should be a subset of the placeholder\n",
        "                    tensors created in add_placeholders.\n",
        "        Hint: When an argument is None, don't add it to the feed_dict.\n",
        "\n",
        "        Args:\n",
        "            inputs_batch: A batch of input data.\n",
        "            labels_batch: A batch of label data.\n",
        "            dropout: The dropout rate.\n",
        "        Returns:\n",
        "            feed_dict: The feed dictionary mapping from placeholders to values.\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE\n",
        "        feed_dict = {\n",
        "            self.input_placeholder: inputs_batch,\n",
        "            self.dropout_placeholder: dropout\n",
        "        }\n",
        "        \n",
        "        if labels_batch is not None:\n",
        "            feed_dict[self.labels_placeholder] = labels_batch\n",
        "            \n",
        "        ### END YOUR CODE\n",
        "        return feed_dict\n",
        "\n",
        "    def add_embedding(self):\n",
        "        \"\"\"Adds an embedding layer that maps from input tokens (integers) to vectors and then\n",
        "        concatenates those vectors:\n",
        "            - Creates a tf.Variable and initializes it with self.pretrained_embeddings.\n",
        "            - Uses the input_placeholder to index into the embeddings tensor, resulting in a\n",
        "              tensor of shape (None, n_features, embedding_size).\n",
        "            - Concatenates the embeddings by reshaping the embeddings tensor to shape\n",
        "              (None, n_features * embedding_size).\n",
        "\n",
        "        Hint: You might find tf.nn.embedding_lookup useful.\n",
        "        Hint: You can use tf.reshape to concatenate the vectors. See following link to understand\n",
        "            what -1 in a shape means.\n",
        "            https://www.tensorflow.org/api_docs/python/tf/reshape\n",
        "\n",
        "        Returns:\n",
        "            embeddings: tf.Tensor of shape (None, n_features*embed_size)\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE\n",
        "        n_features = self.config3.n_features\n",
        "        embedding_size = self.config3.embed_size\n",
        "        \n",
        "        vocabulary = tf.Variable(self.pretrained_embeddings)\n",
        "        embeddings = tf.nn.embedding_lookup(vocabulary, self.input_placeholder)\n",
        "        #Shape: (None, n_features, embedding_size)\n",
        "        #Need to do reshape/concatenate\n",
        "        embeddings = tf.reshape(embeddings, (-1, n_features * embedding_size))\n",
        "        \n",
        "        ### END YOUR CODE\n",
        "        return embeddings\n",
        "\n",
        "    def add_prediction_op(self):\n",
        "        \"\"\"Adds the 1-hidden-layer NN:\n",
        "            h = Relu(xW + b1)\n",
        "            h_drop = Dropout(h, dropout_rate)\n",
        "            pred = h_dropU + b2\n",
        "\n",
        "        Note that we are not applying a softmax to pred. The softmax will instead be done in\n",
        "        the add_loss_op function, which improves efficiency because we can use\n",
        "        tf.nn.softmax_cross_entropy_with_logits\n",
        "\n",
        "        Use the initializer from q2_initialization.py to initialize W and U (you can initialize b1\n",
        "        and b2 with zeros)\n",
        "\n",
        "        Hint: Note that tf.nn.dropout takes the keep probability (1 - p_drop) as an argument.\n",
        "              Therefore the keep probability should be set to the value of\n",
        "              (1 - self.dropout_placeholder)\n",
        "\n",
        "        Returns:\n",
        "            pred: tf.Tensor of shape (batch_size, n_classes)\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.add_embedding()\n",
        "        ### YOUR CODE HERE\n",
        "        #initializer to initialize W and U\n",
        "        xavier_init = xavier_weight_init()\n",
        "        \n",
        "        n_features = self.config3.n_features\n",
        "        n_classes = self.config3.n_classes\n",
        "        embedding_size = self.config3.embed_size\n",
        "        hidden_size = self.config3.hidden_size\n",
        "        \n",
        "        #Hidden_layer\n",
        "        W = tf.Variable(xavier_init((n_features * embedding_size, hidden_size)))\n",
        "        b1 = tf.Variable(tf.zeros((1, hidden_size)))\n",
        "        z = tf.add(tf.matmul(x, W), b1)\n",
        "        h = tf.nn.relu(z)\n",
        "        \n",
        "        h_drop = tf.nn.dropout(h, keep_prob = 1 - self.dropout_placeholder)\n",
        "        \n",
        "        #Output_layer\n",
        "        U = tf.Variable(xavier_init((hidden_size, n_classes)))\n",
        "        b2 = tf.Variable(tf.zeros((1, n_classes)))\n",
        "        \n",
        "        pred = tf.add(tf.matmul(h_drop, U), b2)\n",
        "        \n",
        "        ### END YOUR CODE\n",
        "        return pred\n",
        "\n",
        "    def add_loss_op(self, pred):\n",
        "        \"\"\"Adds Ops for the loss function to the computational graph.\n",
        "        In this case we are using cross entropy loss.\n",
        "        The loss should be averaged over all examples in the current minibatch.\n",
        "\n",
        "        Hint: You can use tf.nn.softmax_cross_entropy_with_logits to simplify your\n",
        "                    implementation. You might find tf.reduce_mean useful.\n",
        "        Args:\n",
        "            pred: A tensor of shape (batch_size, n_classes) containing the output of the neural\n",
        "                  network before the softmax layer.\n",
        "        Returns:\n",
        "            loss: A 0-d tensor (scalar)\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE\n",
        "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.labels_placeholder, \n",
        "                                                                      logits = pred))\n",
        "        \n",
        "        ### END YOUR CODE\n",
        "        return loss\n",
        "\n",
        "    def add_training_op(self, loss):\n",
        "        \"\"\"Sets up the training Ops.\n",
        "\n",
        "        Creates an optimizer and applies the gradients to all trainable variables.\n",
        "        The Op returned by this function is what must be passed to the\n",
        "        `sess.run()` call to cause the model to train. See\n",
        "\n",
        "        https://www.tensorflow.org/api_docs/python/tf/train/Optimizer\n",
        "\n",
        "        for more information.\n",
        "\n",
        "        Use tf.train.AdamOptimizer for this model.\n",
        "        Use the learning rate from self.config.\n",
        "        Calling optimizer.minimize() will return a train_op object.\n",
        "\n",
        "        Args:\n",
        "            loss: Loss tensor, from cross_entropy_loss.\n",
        "        Returns:\n",
        "            train_op: The Op for training.\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE\n",
        "        optimizer = tf.train.AdamOptimizer(self.config3.lr)\n",
        "        train_op = optimizer.minimize(loss)\n",
        "        \n",
        "        ### END YOUR CODE\n",
        "        return train_op\n",
        "\n",
        "    def train_on_batch(self, sess, inputs_batch, labels_batch):\n",
        "        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch,\n",
        "                                     dropout=self.config3.dropout)\n",
        "        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)\n",
        "        return loss\n",
        "\n",
        "    def run_epoch(self, sess, parser, train_examples, dev_set):\n",
        "        n_minibatches = 1 + len(train_examples) / self.config3.batch_size\n",
        "        prog = tf.keras.utils.Progbar(target=n_minibatches)\n",
        "        for i, (train_x, train_y) in enumerate(minibatches(train_examples, self.config3.batch_size)):\n",
        "            loss = self.train_on_batch(sess, train_x, train_y)\n",
        "            prog.update(i + 1, [(\"train loss\", loss)], force=i + 1 == n_minibatches)\n",
        "\n",
        "        print(\"  Evaluating on dev set\",)\n",
        "        dev_UAS, _ = parser.parse(dev_set)\n",
        "        print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
        "        return dev_UAS\n",
        "\n",
        "    def fit(self, sess, saver, parser, train_examples, dev_set):\n",
        "        best_dev_UAS = 0\n",
        "        for epoch in range(self.config3.n_epochs):\n",
        "            print( \"Epoch {:} out of {:}\".format(epoch + 1, self.config3.n_epochs))\n",
        "            dev_UAS = self.run_epoch(sess, parser, train_examples, dev_set)\n",
        "            if dev_UAS > best_dev_UAS:\n",
        "                best_dev_UAS = dev_UAS\n",
        "                if saver:\n",
        "                    print( \"New best dev UAS! Saving model in ./data/weights/parser.weights\")\n",
        "                    saver.save(sess, './data/weights/parser.weights')\n",
        "            print()\n",
        "\n",
        "    def __init__(self, config3, pretrained_embeddings):\n",
        "        self.pretrained_embeddings = pretrained_embeddings\n",
        "        self.config3 = config3\n",
        "        self.build()\n",
        "\n",
        "\n",
        "def main(debug=False):\n",
        "    print( 80 * \"=\")\n",
        "    print( \"INITIALIZING\")\n",
        "    print( 80 * \"=\")\n",
        "    config3 = Config3()\n",
        "    parser, embeddings, train_examples, dev_set, test_set = load_and_preprocess_data(debug)\n",
        "    if not os.path.exists('./data/weights/'):\n",
        "        os.makedirs('./data/weights/')\n",
        "\n",
        "    with tf.Graph().as_default() as graph:\n",
        "        print( \"Building model...\",)\n",
        "        start = time.time()\n",
        "        model = ParserModel(config3, embeddings)\n",
        "        parser.model = model\n",
        "        init_op = tf.global_variables_initializer()\n",
        "        saver = None if debug else tf.train.Saver()\n",
        "        print( \"took {:.2f} seconds\\n\".format(time.time() - start))\n",
        "    graph.finalize()\n",
        "\n",
        "    with tf.Session(graph=graph) as session:\n",
        "        parser.session = session\n",
        "        session.run(init_op)\n",
        "\n",
        "        print( 80 * \"=\")\n",
        "        print( \"TRAINING\")\n",
        "        print( 80 * \"=\")\n",
        "        model.fit(session, saver, parser, train_examples, dev_set)\n",
        "\n",
        "        if not debug:\n",
        "            print( 80 * \"=\")\n",
        "            print( \"TESTING\")\n",
        "            print( 80 * \"=\")\n",
        "            print( \"Restoring the best model weights found on the dev set\")\n",
        "            saver.restore(session, './data/weights/parser.weights')\n",
        "            print( \"Final evaluation on test set\",)\n",
        "            UAS, dependencies = parser.parse(test_set)\n",
        "            print( \"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
        "            print( \"Writing predictions\")\n",
        "            with open('q2_test.predicted.pkl', 'wb') as f:\n",
        "                cPickle.dump(dependencies, f, -1)\n",
        "            folder = '1CrxPRgbU-lQamwg--9FuuJWrDd5plC0z'\n",
        "            f = drive.CreateFile({\"parents\": [{'title': 'q2_test.predicted.pkl', \n",
        "                                               \"kind\": \"drive#fileLink\", \n",
        "                                               \"id\": folder}]})\n",
        "            f.SetContentFile('q2_test.predicted.pkl')\n",
        "            f.Upload()\n",
        "            \n",
        "            print( \"Done!\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "INITIALIZING\n",
            "================================================================================\n",
            "Loading data...\n",
            "took 2.02 seconds\n",
            "Building parser...\n",
            "took 1.65 seconds\n",
            "Loading pretrained embeddings...\n",
            "took 2.94 seconds\n",
            "Vectorizing data...\n",
            "took 1.65 seconds\n",
            "Preprocessing training data...\n",
            "took 56.68 seconds\n",
            "Building model...\n",
            "took 0.25 seconds\n",
            "\n",
            "================================================================================\n",
            "TRAINING\n",
            "================================================================================\n",
            "Epoch 1 out of 15\n",
            " 823/1848 [============>.................] 823/1848 [============>.................] - ETA: 15s - train loss: 0.2351"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1843/1848 [============================>.]1843/1848 [============================>.] - ETA: 0s - train loss: 0.1820  Evaluating on dev set\n",
            "- dev UAS: 84.37\n",
            "New best dev UAS! Saving model in ./data/weights/parser.weights\n",
            "\n",
            "Epoch 2 out of 15\n",
            "1455/1848 [======================>.......]1455/1848 [======================>.......] - ETA: 4s - train loss: 0.1156"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1844/1848 [============================>.]1844/1848 [============================>.] - ETA: 0s - train loss: 0.1143  Evaluating on dev set\n",
            "- dev UAS: 86.52\n",
            "New best dev UAS! Saving model in ./data/weights/parser.weights\n",
            "\n",
            "Epoch 3 out of 15\n",
            "1741/1848 [===========================>..]1741/1848 [===========================>..] - ETA: 1s - train loss: 0.1003"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1846/1848 [============================>.]1846/1848 [============================>.] - ETA: 0s - train loss: 0.1001  Evaluating on dev set\n",
            "- dev UAS: 86.99\n",
            "New best dev UAS! Saving model in ./data/weights/parser.weights\n",
            "\n",
            "Epoch 4 out of 15\n",
            "1739/1848 [===========================>..]1739/1848 [===========================>..] - ETA: 1s - train loss: 0.0917"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1842/1848 [============================>.]1842/1848 [============================>.] - ETA: 0s - train loss: 0.0916  Evaluating on dev set\n",
            "- dev UAS: 87.44\n",
            "New best dev UAS! Saving model in ./data/weights/parser.weights\n",
            "\n",
            "Epoch 5 out of 15\n",
            "1814/1848 [============================>.]1814/1848 [============================>.] - ETA: 0s - train loss: 0.0854"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1848/1848 [============================>.]1848/1848 [============================>.] - ETA: 0s - train loss: 0.0853  Evaluating on dev set\n",
            "- dev UAS: 88.10\n",
            "New best dev UAS! Saving model in ./data/weights/parser.weights\n",
            "\n",
            "Epoch 6 out of 15\n",
            "1800/1848 [============================>.]1800/1848 [============================>.] - ETA: 0s - train loss: 0.0801"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1844/1848 [============================>.]1844/1848 [============================>.] - ETA: 0s - train loss: 0.0802  Evaluating on dev set\n",
            "- dev UAS: 87.77\n",
            "\n",
            "Epoch 7 out of 15\n",
            "1844/1848 [============================>.]1844/1848 [============================>.] - ETA: 0s - train loss: 0.0761  Evaluating on dev set\n",
            "- dev UAS: 88.54\n",
            "New best dev UAS! Saving model in ./data/weights/parser.weights\n",
            "\n",
            "Epoch 8 out of 15\n",
            " 193/1848 [==>...........................] 193/1848 [==>...........................] - ETA: 57s - train loss: 0.0711"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1847/1848 [============================>.]1847/1848 [============================>.] - ETA: 0s - train loss: 0.0724  Evaluating on dev set\n",
            "- dev UAS: 88.41\n",
            "\n",
            "Epoch 9 out of 15\n",
            "1621/1848 [=========================>....]1621/1848 [=========================>....] - ETA: 2s - train loss: 0.0693"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1843/1848 [============================>.]1843/1848 [============================>.] - ETA: 0s - train loss: 0.0693  Evaluating on dev set\n",
            "- dev UAS: 88.63\n",
            "New best dev UAS! Saving model in ./data/weights/parser.weights\n",
            "\n",
            "Epoch 10 out of 15\n",
            "1705/1848 [==========================>...]1705/1848 [==========================>...] - ETA: 1s - train loss: 0.0666"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1842/1848 [============================>.]1842/1848 [============================>.] - ETA: 0s - train loss: 0.0666  Evaluating on dev set\n",
            "- dev UAS: 88.82\n",
            "New best dev UAS! Saving model in ./data/weights/parser.weights\n",
            "\n",
            "Epoch 11 out of 15\n",
            "1797/1848 [============================>.]1797/1848 [============================>.] - ETA: 0s - train loss: 0.0638"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1845/1848 [============================>.]1845/1848 [============================>.] - ETA: 0s - train loss: 0.0639  Evaluating on dev set\n",
            "- dev UAS: 88.55\n",
            "\n",
            "Epoch 12 out of 15\n",
            "1848/1848 [============================>.]1848/1848 [============================>.] - ETA: 0s - train loss: 0.0616  Evaluating on dev set\n",
            "- dev UAS: 89.07\n",
            "New best dev UAS! Saving model in ./data/weights/parser.weights\n",
            "\n",
            "Epoch 13 out of 15\n",
            " 168/1848 [=>............................] 168/1848 [=>............................] - ETA: 1:04 - train loss: 0.0567"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1848/1848 [============================>.]1848/1848 [============================>.] - ETA: 0s - train loss: 0.0595  Evaluating on dev set\n",
            "- dev UAS: 88.73\n",
            "\n",
            "Epoch 14 out of 15\n",
            "1581/1848 [========================>.....]1581/1848 [========================>.....] - ETA: 3s - train loss: 0.0568"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1843/1848 [============================>.]1843/1848 [============================>.] - ETA: 0s - train loss: 0.0569  Evaluating on dev set\n",
            "- dev UAS: 88.87\n",
            "\n",
            "Epoch 15 out of 15\n",
            "1842/1848 [============================>.]1842/1848 [============================>.] - ETA: 0s - train loss: 0.0554  Evaluating on dev set\n",
            "- dev UAS: 88.83\n",
            "\n",
            "================================================================================\n",
            "TESTING\n",
            "================================================================================\n",
            "Restoring the best model weights found on the dev set\n",
            "INFO:tensorflow:Restoring parameters from ./data/weights/parser.weights\n",
            "Final evaluation on test set\n",
            "- test UAS: 89.43\n",
            "Writing predictions\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rx_enIxi-xVG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How to create files on Google Drive with Python (don't know how to set root path......)"
      ]
    },
    {
      "metadata": {
        "id": "277q0WO18c7G",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# Create a local file to upload.\n",
        "with open('/tmp/to_upload.txt', 'w') as f:\n",
        "    f.write('my sample file')\n",
        "\n",
        "print('/tmp/to_upload.txt contains:')\n",
        "!cat /tmp/to_upload.txt\n",
        "\n",
        "# Upload the file to Drive. See:\n",
        "#\n",
        "# https://developers.google.com/drive/v3/reference/files/create\n",
        "# https://developers.google.com/drive/v3/web/manage-uploads\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "\n",
        "file_metadata = {\n",
        "  'name': 'Sample file',\n",
        "  'mimeType': 'text/plain'\n",
        "}\n",
        "media = MediaFileUpload('/tmp/to_upload.txt', \n",
        "                        mimetype='text/plain',\n",
        "                        resumable=True)\n",
        "created = drive_service.files().create(body=file_metadata,\n",
        "                                       media_body=media,\n",
        "                                       fields='id').execute()\n",
        "print('File ID: {}'.format(created.get('id')))\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QfYuGV6q8Nxf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "51a1bb77-0338-4dbb-f69e-d5468c80f4d2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523723675230,
          "user_tz": -480,
          "elapsed": 711,
          "user": {
            "displayName": "姚坤武",
            "photoUrl": "//lh4.googleusercontent.com/-nv4q5H1gHI4/AAAAAAAAAAI/AAAAAAAAiAs/AzMKglvE7Bk/s50-c-k-no/photo.jpg",
            "userId": "104666213213814627780"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#for file1 in file_list:       \n",
        "#    print ('title: %s, id: %s' % (file1['title'], file1['id']))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: dev.conll, id: 1qQdaVA7vltM7k_mAEA7BfosExu7jEqR2\n",
            "title: weight, id: 1CrxPRgbU-lQamwg--9FuuJWrDd5plC0z\n",
            "title: weight, id: 16zfXFh4xDR_GB1K_X_aG_wJMfsFK5v8T\n",
            "title: train.gold.conll, id: 1DAh9a83MRgqv95eaj3LTUZA4KLK2zlY_\n",
            "title: train.conll, id: 155yq13fVwHypPVicbZdtOigSu8sQt19z\n",
            "title: test.gold.conll, id: 1eXvxO5y52kZuE5P8Ruz8RuFKKVjD0aHi\n",
            "title: test.conll, id: 1Dgy3UZNyB_beX6JSh_5HrkjaTD_R_ewc\n",
            "title: en-cw.txt, id: 1stVKwKmn6lO7LQz8jPqcwBaRwCEyTV4-\n",
            "title: dev.gold.conll, id: 119gX_LgivBhTcfkbBXpnNIJ0OUauq4j7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ROLKsXpj8G52",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#https://raspberrypi.stackexchange.com/questions/14186/use-pydrive-to-upload-files-to-google-drive-folder\n",
        "fid = '1CrxPRgbU-lQamwg--9FuuJWrDd5plC0z'\n",
        "f = drive.CreateFile({\"parents\": [{\"kind\": \"drive#fileLink\", \"id\": fid}]})\n",
        "f.SetContentFile( 'en-cw.txt' )\n",
        "f.Upload()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}