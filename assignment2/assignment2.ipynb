{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KunWuYao/anaconda/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax test 1 passed!\n",
      "Softmax test 2 passed!\n",
      "Basic (non-exhaustive) softmax tests pass\n",
      "\n",
      "Cross-entropy test 1 passed!\n",
      "Basic (non-exhaustive) cross-entropy tests pass\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "#print(os.getcwd())\n",
    "os.chdir('/Users/KunWuYao/GitHub/NLP - Stanford/assignment2/')\n",
    "from utils.general_utils import test_all_close\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax function in tensorflow.\n",
    "\n",
    "    You might find the tensorflow functions tf.exp, tf.reduce_max,\n",
    "    tf.reduce_sum, tf.expand_dims useful. (Many solutions are possible, so you may\n",
    "    not need to use all of these functions). Recall also that many common\n",
    "    tensorflow operations are sugared (e.g. x + y does elementwise addition\n",
    "    if x and y are both tensors). Make sure to implement the numerical stability\n",
    "    fixes as in the previous homework!\n",
    "\n",
    "    Args:\n",
    "        x:   tf.Tensor with shape (n_samples, n_features). Note feature vectors are\n",
    "                  represented by row-vectors. (For simplicity, no need to handle 1-d\n",
    "                  input as in the previous homework)\n",
    "    Returns:\n",
    "        out: tf.Tensor with shape (n_sample, n_features). You need to construct this\n",
    "                  tensor in this problem.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    a = tf.subtract(x, tf.reduce_max(x, 1, keep_dims = True))\n",
    "    e = tf.exp(a)\n",
    "    s = tf.reduce_sum(e,1, keep_dims = True)\n",
    "    \n",
    "    out = e / s\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y, yhat):\n",
    "    \"\"\"\n",
    "    Compute the cross entropy loss in tensorflow.\n",
    "    The loss should be summed over the current minibatch.\n",
    "\n",
    "    y is a one-hot tensor of shape (n_samples, n_classes) and yhat is a tensor\n",
    "    of shape (n_samples, n_classes). y should be of dtype tf.int32, and yhat should\n",
    "    be of dtype tf.float32.\n",
    "\n",
    "    The functions tf.to_float, tf.reduce_sum, and tf.log might prove useful. (Many\n",
    "    solutions are possible, so you may not need to use all of these functions).\n",
    "\n",
    "    Note: You are NOT allowed to use the tensorflow built-in cross-entropy\n",
    "                functions.\n",
    "\n",
    "    Args:\n",
    "        y:    tf.Tensor with shape (n_samples, n_classes). One-hot encoded.\n",
    "        yhat: tf.Tensorwith shape (n_sample, n_classes). Each row encodes a\n",
    "                    probability distribution and should sum to 1.\n",
    "    Returns:\n",
    "        out:  tf.Tensor with shape (1,) (Scalar output). You need to construct this\n",
    "                    tensor in the problem.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    y = tf.to_float(y)\n",
    "    out = -tf.reduce_sum(tf.multiply(y, tf.log(yhat)))\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests of softmax to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "\n",
    "    test1 = softmax(tf.constant(np.array([[1001, 1002], [3, 4]]), dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "        test1 = sess.run(test1)\n",
    "    test_all_close(\"Softmax test 1\", test1, np.array([[0.26894142, 0.73105858],\n",
    "                                                      [0.26894142, 0.73105858]]))\n",
    "\n",
    "    test2 = softmax(tf.constant(np.array([[-1001, -1002]]), dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "        test2 = sess.run(test2)\n",
    "    test_all_close(\"Softmax test 2\", test2, np.array([[0.73105858, 0.26894142]]))\n",
    "\n",
    "    print(\"Basic (non-exhaustive) softmax tests pass\\n\")\n",
    "\n",
    "\n",
    "def test_cross_entropy_loss_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests of cross_entropy_loss to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    y = np.array([[0, 1], [1, 0], [1, 0]])\n",
    "    yhat = np.array([[.5, .5], [.5, .5], [.5, .5]])\n",
    "\n",
    "    test1 = cross_entropy_loss(tf.constant(y, dtype=tf.int32), tf.constant(yhat, dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "        test1 = sess.run(test1)\n",
    "    expected = -3 * np.log(0.5)\n",
    "    test_all_close(\"Cross-entropy test 1\", test1, expected)\n",
    "\n",
    "    print(\"Basic (non-exhaustive) cross-entropy tests pass\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_softmax_basic()\n",
    "    test_cross_entropy_loss_basic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 59.18 (0.039 sec)\n",
      "Epoch 1: loss = 20.32 (0.016 sec)\n",
      "Epoch 2: loss = 10.92 (0.019 sec)\n",
      "Epoch 3: loss = 7.30 (0.021 sec)\n",
      "Epoch 4: loss = 5.44 (0.023 sec)\n",
      "Epoch 5: loss = 4.32 (0.022 sec)\n",
      "Epoch 6: loss = 3.58 (0.020 sec)\n",
      "Epoch 7: loss = 3.05 (0.019 sec)\n",
      "Epoch 8: loss = 2.65 (0.022 sec)\n",
      "Epoch 9: loss = 2.35 (0.020 sec)\n",
      "Epoch 10: loss = 2.11 (0.019 sec)\n",
      "Epoch 11: loss = 1.91 (0.025 sec)\n",
      "Epoch 12: loss = 1.75 (0.021 sec)\n",
      "Epoch 13: loss = 1.61 (0.022 sec)\n",
      "Epoch 14: loss = 1.49 (0.021 sec)\n",
      "Epoch 15: loss = 1.39 (0.026 sec)\n",
      "Epoch 16: loss = 1.30 (0.026 sec)\n",
      "Epoch 17: loss = 1.22 (0.016 sec)\n",
      "Epoch 18: loss = 1.15 (0.019 sec)\n",
      "Epoch 19: loss = 1.09 (0.018 sec)\n",
      "Epoch 20: loss = 1.03 (0.022 sec)\n",
      "Epoch 21: loss = 0.98 (0.019 sec)\n",
      "Epoch 22: loss = 0.94 (0.025 sec)\n",
      "Epoch 23: loss = 0.89 (0.021 sec)\n",
      "Epoch 24: loss = 0.86 (0.017 sec)\n",
      "Epoch 25: loss = 0.82 (0.018 sec)\n",
      "Epoch 26: loss = 0.79 (0.014 sec)\n",
      "Epoch 27: loss = 0.76 (0.022 sec)\n",
      "Epoch 28: loss = 0.73 (0.020 sec)\n",
      "Epoch 29: loss = 0.71 (0.025 sec)\n",
      "Epoch 30: loss = 0.68 (0.015 sec)\n",
      "Epoch 31: loss = 0.66 (0.015 sec)\n",
      "Epoch 32: loss = 0.64 (0.016 sec)\n",
      "Epoch 33: loss = 0.62 (0.014 sec)\n",
      "Epoch 34: loss = 0.60 (0.020 sec)\n",
      "Epoch 35: loss = 0.58 (0.019 sec)\n",
      "Epoch 36: loss = 0.57 (0.026 sec)\n",
      "Epoch 37: loss = 0.55 (0.022 sec)\n",
      "Epoch 38: loss = 0.54 (0.016 sec)\n",
      "Epoch 39: loss = 0.52 (0.017 sec)\n",
      "Epoch 40: loss = 0.51 (0.019 sec)\n",
      "Epoch 41: loss = 0.50 (0.022 sec)\n",
      "Epoch 42: loss = 0.48 (0.020 sec)\n",
      "Epoch 43: loss = 0.47 (0.018 sec)\n",
      "Epoch 44: loss = 0.46 (0.018 sec)\n",
      "Epoch 45: loss = 0.45 (0.018 sec)\n",
      "Epoch 46: loss = 0.44 (0.018 sec)\n",
      "Epoch 47: loss = 0.43 (0.025 sec)\n",
      "Epoch 48: loss = 0.42 (0.023 sec)\n",
      "Epoch 49: loss = 0.41 (0.019 sec)\n",
      "Basic (non-exhaustive) classifier tests pass\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from q1_softmax import softmax\n",
    "from q1_softmax import cross_entropy_loss\n",
    "from model import Model\n",
    "from utils.general_utils import get_minibatches\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "    The config class is used to store various hyperparameters and dataset\n",
    "    information parameters. Model objects are passed a Config() object at\n",
    "    instantiation. They can then call self.config.<hyperparameter_name> to\n",
    "    get the hyperparameter settings.\n",
    "    \"\"\"\n",
    "    n_samples = 1024\n",
    "    n_features = 100\n",
    "    n_classes = 5\n",
    "    batch_size = 64\n",
    "    n_epochs = 50\n",
    "    lr = 1e-4\n",
    "\n",
    "\n",
    "class SoftmaxModel(Model):\n",
    "    \"\"\"Implements a Softmax classifier with cross-entropy loss.\"\"\"\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Generates placeholder variables to represent the input tensors.\n",
    "\n",
    "        These placeholders are used as inputs by the rest of the model building\n",
    "        and will be fed data during training.\n",
    "\n",
    "        Adds following nodes to the computational graph\n",
    "\n",
    "        input_placeholder: Input placeholder tensor of shape\n",
    "                                              (batch_size, n_features), type tf.float32\n",
    "        labels_placeholder: Labels placeholder tensor of shape\n",
    "                                              (batch_size, n_classes), type tf.int32\n",
    "\n",
    "        Add these placeholders to self as the instance variables\n",
    "            self.input_placeholder\n",
    "            self.labels_placeholder\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        n_features = self.config.n_features\n",
    "        n_classes = self.config.n_classes\n",
    "        \n",
    "        self.input_placeholder = tf.placeholder(tf.float32, shape = (None, n_features))\n",
    "        self.labels_placeholder = tf.placeholder(tf.int32, shape = (None, n_classes))\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def create_feed_dict(self, inputs_batch, labels_batch=None):\n",
    "        \"\"\"Creates the feed_dict for training the given step.\n",
    "\n",
    "        A feed_dict takes the form of:\n",
    "        feed_dict = {\n",
    "                <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "                ....\n",
    "        }\n",
    "\n",
    "        If label_batch is None, then no labels are added to feed_dict.\n",
    "\n",
    "        Hint: The keys for the feed_dict should be the placeholder\n",
    "                tensors created in add_placeholders.\n",
    "\n",
    "        Args:\n",
    "            inputs_batch: A batch of input data.\n",
    "            labels_batch: A batch of label data.\n",
    "        Returns:\n",
    "            feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        feed_dict = {\n",
    "            self.input_placeholder: inputs_batch,\n",
    "            self.labels_placeholder: labels_batch\n",
    "        }\n",
    "        ### END YOUR CODE\n",
    "        return feed_dict\n",
    "\n",
    "    def add_prediction_op(self):\n",
    "        \"\"\"Adds the core transformation for this model which transforms a batch of input\n",
    "        data into a batch of predictions. In this case, the transformation is a linear layer plus a\n",
    "        softmax transformation:\n",
    "\n",
    "        yhat = softmax(xW + b)\n",
    "\n",
    "        Hint: The input x will be passed in through self.input_placeholder. Each ROW of\n",
    "              self.input_placeholder is a single example. This is usually best-practice for\n",
    "              tensorflow code.\n",
    "        Hint: Make sure to create tf.Variables as needed.\n",
    "        Hint: For this simple use-case, it's sufficient to initialize both weights W\n",
    "                    and biases b with zeros.\n",
    "\n",
    "        Returns:\n",
    "            pred: A tensor of shape (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        n_features = self.config.n_features\n",
    "        n_classes = self.config.n_classes\n",
    "        \n",
    "        W = tf.Variable(tf.zeros([n_features, n_classes]))\n",
    "        b = tf.Variable(tf.zeros([n_classes]))\n",
    "        \n",
    "        z = tf.add(tf.matmul(self.input_placeholder, W), b)\n",
    "        pred = softmax(z)\n",
    "        ### END YOUR CODE\n",
    "        return pred\n",
    "\n",
    "    def add_loss_op(self, pred):\n",
    "        \"\"\"Adds cross_entropy_loss ops to the computational graph.\n",
    "\n",
    "        Hint: Use the cross_entropy_loss function we defined. This should be a very\n",
    "                    short function.\n",
    "        Args:\n",
    "            pred: A tensor of shape (batch_size, n_classes)\n",
    "        Returns:\n",
    "            loss: A 0-d tensor (scalar)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        loss = cross_entropy_loss(self.labels_placeholder, pred)\n",
    "        ### END YOUR CODE\n",
    "        return loss\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        `sess.run()` call to cause the model to train. See\n",
    "\n",
    "        https://www.tensorflow.org/api_docs/python/tf/train/Optimizer\n",
    "\n",
    "        for more information. Use the learning rate from self.config.\n",
    "\n",
    "        Hint: Use tf.train.GradientDescentOptimizer to get an optimizer object.\n",
    "                    Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "        Args:\n",
    "            loss: Loss tensor, from cross_entropy_loss.\n",
    "        Returns:\n",
    "            train_op: The Op for training.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.config.lr)\n",
    "        train_op = optimizer.minimize(loss)\n",
    "        ### END YOUR CODE\n",
    "        return train_op\n",
    "\n",
    "    def run_epoch(self, sess, inputs, labels):\n",
    "        \"\"\"Runs an epoch of training.\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session() object\n",
    "            inputs: np.ndarray of shape (n_samples, n_features)\n",
    "            labels: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "            average_loss: scalar. Average minibatch loss of model on epoch.\n",
    "        \"\"\"\n",
    "        n_minibatches, total_loss = 0, 0\n",
    "        for input_batch, labels_batch in get_minibatches([inputs, labels], self.config.batch_size):\n",
    "            n_minibatches += 1\n",
    "            total_loss += self.train_on_batch(sess, input_batch, labels_batch)\n",
    "        return total_loss / n_minibatches\n",
    "\n",
    "    def fit(self, sess, inputs, labels):\n",
    "        \"\"\"Fit model on provided data.\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session()\n",
    "            inputs: np.ndarray of shape (n_samples, n_features)\n",
    "            labels: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "            losses: list of loss per epoch\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        for epoch in range(self.config.n_epochs):\n",
    "            start_time = time.time()\n",
    "            average_loss = self.run_epoch(sess, inputs, labels)\n",
    "            duration = time.time() - start_time\n",
    "            print ('Epoch {:}: loss = {:.2f} ({:.3f} sec)'.format(epoch, average_loss, duration))\n",
    "            losses.append(average_loss)\n",
    "        return losses\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initializes the model.\n",
    "\n",
    "        Args:\n",
    "            config: A model configuration object of type Config\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.build()\n",
    "\n",
    "\n",
    "def test_softmax_model():\n",
    "    \"\"\"Train softmax model for a number of steps.\"\"\"\n",
    "    config = Config()\n",
    "\n",
    "    # Generate random data to train the model on\n",
    "    np.random.seed(1234)\n",
    "    inputs = np.random.rand(config.n_samples, config.n_features)\n",
    "    labels = np.zeros((config.n_samples, config.n_classes), dtype=np.int32)\n",
    "    labels[:, 0] = 1\n",
    "\n",
    "    # Tell TensorFlow that the model will be built into the default Graph.\n",
    "    # (not required but good practice)\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        # Build the model and add the variable initializer op\n",
    "        model = SoftmaxModel(config)\n",
    "        init_op = tf.global_variables_initializer()\n",
    "    # Finalizing the graph causes tensorflow to raise an exception if you try to modify the graph\n",
    "    # further. This is good practice because it makes explicit the distinction between building and\n",
    "    # running the graph.\n",
    "    graph.finalize()\n",
    "\n",
    "    # Create a session for running ops in the graph\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # Run the op to initialize the variables.\n",
    "        sess.run(init_op)\n",
    "        # Fit the model\n",
    "        losses = model.fit(sess, inputs, labels)\n",
    "\n",
    "    # If ops are implemented correctly, the average loss should fall close to zero\n",
    "    # rapidly.\n",
    "    assert losses[-1] < .5\n",
    "    print (\"Basic (non-exhaustive) classifier tests pass\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_softmax_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHIFT test passed!\n",
      "LEFT-ARC test passed!\n",
      "RIGHT-ARC test passed!\n",
      "parse test passed!\n",
      "minibatch_parse test passed!\n"
     ]
    }
   ],
   "source": [
    "class PartialParse(object):\n",
    "    def __init__(self, sentence):\n",
    "        \"\"\"Initializes this partial parse.\n",
    "\n",
    "        Your code should initialize the following fields:\n",
    "            self.stack: The current stack represented as a list with the top of the stack as the\n",
    "                        last element of the list.\n",
    "            self.buffer: The current buffer represented as a list with the first item on the\n",
    "                         buffer as the first item of the list\n",
    "            self.dependencies: The list of dependencies produced so far. Represented as a list of\n",
    "                    tuples where each tuple is of the form (head, dependent).\n",
    "                    Order for this list doesn't matter.\n",
    "\n",
    "        The root token should be represented with the string \"ROOT\"\n",
    "\n",
    "        Args:\n",
    "            sentence: The sentence to be parsed as a list of words.\n",
    "                      Your code should not modify the sentence.\n",
    "        \"\"\"\n",
    "        # The sentence being parsed is kept for bookkeeping purposes. Do not use it in your code.\n",
    "        self.sentence = sentence\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        self.stack = [\"ROOT\"]\n",
    "        self.buffer = [word for word in sentence]\n",
    "        self.dependencies = []\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def parse_step(self, transition):\n",
    "        \"\"\"Performs a single parse step by applying the given transition to this partial parse\n",
    "\n",
    "        Args:\n",
    "            transition: A string that equals \"S\", \"LA\", or \"RA\" representing the shift, left-arc,\n",
    "                        and right-arc transitions. You can assume the provided transition is a legal\n",
    "                        transition.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        if transition == 'S':\n",
    "            self.stack.append(self.buffer.pop(0))\n",
    "        elif transition == 'LA':\n",
    "            dependent, head = self.stack[-2:]\n",
    "            #Remove the dependent from the stack, keep the head.\n",
    "            self.stack.pop(-2)\n",
    "            self.dependencies.append((head, dependent))\n",
    "        elif transition == 'RA':\n",
    "            head, dependent = self.stack[-2:]\n",
    "            self.stack.pop(-1)\n",
    "            self.dependencies.append((head, dependent))\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def parse(self, transitions):\n",
    "        \"\"\"Applies the provided transitions to this PartialParse\n",
    "\n",
    "        Args:\n",
    "            transitions: The list of transitions in the order they should be applied\n",
    "        Returns:\n",
    "            dependencies: The list of dependencies produced when parsing the sentence. Represented\n",
    "                          as a list of tuples where each tuple is of the form (head, dependent)\n",
    "        \"\"\"\n",
    "        for transition in transitions:\n",
    "            self.parse_step(transition)\n",
    "        return self.dependencies\n",
    "\n",
    "\n",
    "def minibatch_parse(sentences, model, batch_size):\n",
    "    \"\"\"Parses a list of sentences in minibatches using a model.\n",
    "\n",
    "    Args:\n",
    "        sentences: A list of sentences to be parsed (each sentence is a list of words)\n",
    "        model: The model that makes parsing decisions. It is assumed to have a function\n",
    "               model.predict(partial_parses) that takes in a list of PartialParses as input and\n",
    "               returns a list of transitions predicted for each parse. That is, after calling\n",
    "                   transitions = model.predict(partial_parses)\n",
    "               transitions[i] will be the next transition to apply to partial_parses[i].\n",
    "        batch_size: The number of PartialParses to include in each minibatch\n",
    "    Returns:\n",
    "        dependencies: A list where each element is the dependencies list for a parsed sentence.\n",
    "                      Ordering should be the same as in sentences (i.e., dependencies[i] should\n",
    "                      contain the parse for sentences[i]).\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    #[[sentence (stack, buffer, dependencies)], [sentence (stack, buffer, dependencies)]]\n",
    "    partial_parses = [PartialParse(sentence) for sentence in sentences]\n",
    "    unfinished_parses = partial_parses[:]\n",
    "    \n",
    "    while unfinished_parses:\n",
    "        #Set minibatch size and content\n",
    "        minibatch_parses = unfinished_parses[:batch_size]\n",
    "        transitions = model.predict(minibatch_parses)\n",
    "        \n",
    "        #run through the minibatch items\n",
    "        #if not done / no minibatch_parses deleted, keep running with the while loop!\n",
    "        for parse, transition in zip(minibatch_parses, transitions):\n",
    "            #Parse is a sentence (Class, has self.stack, self.buffer, self.dependencies)\n",
    "            parse.parse_step(transition)\n",
    "            if len(parse.stack) < 2 and len(parse.buffer) < 1:\n",
    "                unfinished_parses.remove(parse)\n",
    "    \n",
    "    dependencies = [p.dependencies for p in partial_parses]\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return dependencies\n",
    "\n",
    "\n",
    "def test_step(name, transition, stack, buf, deps,\n",
    "              ex_stack, ex_buf, ex_deps):\n",
    "    \"\"\"Tests that a single parse step returns the expected output\"\"\"\n",
    "    pp = PartialParse([])\n",
    "    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n",
    "\n",
    "    pp.parse_step(transition)\n",
    "    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer), tuple(sorted(pp.dependencies)))\n",
    "    assert stack == ex_stack, \\\n",
    "        \"{:} test resulted in stack {:}, expected {:}\".format(name, stack, ex_stack)\n",
    "    assert buf == ex_buf, \\\n",
    "        \"{:} test resulted in buffer {:}, expected {:}\".format(name, buf, ex_buf)\n",
    "    assert deps == ex_deps, \\\n",
    "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
    "    print(\"{:} test passed!\".format(name))\n",
    "\n",
    "\n",
    "def test_parse_step():\n",
    "    \"\"\"Simple tests for the PartialParse.parse_step function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    test_step(\"SHIFT\", \"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [],\n",
    "              (\"ROOT\", \"the\", \"cat\"), (\"sat\",), ())\n",
    "    test_step(\"LEFT-ARC\", \"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [],\n",
    "              (\"ROOT\", \"cat\",), (\"sat\",), ((\"cat\", \"the\"),))\n",
    "    test_step(\"RIGHT-ARC\", \"RA\", [\"ROOT\", \"run\", \"fast\"], [], [],\n",
    "              (\"ROOT\", \"run\",), (), ((\"run\", \"fast\"),))\n",
    "\n",
    "\n",
    "def test_parse():\n",
    "    \"\"\"Simple tests for the PartialParse.parse function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    sentence = [\"parse\", \"this\", \"sentence\"]\n",
    "    dependencies = PartialParse(sentence).parse([\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n",
    "    dependencies = tuple(sorted(dependencies))\n",
    "    expected = (('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))\n",
    "    assert dependencies == expected,  \\\n",
    "        \"parse test resulted in dependencies {:}, expected {:}\".format(dependencies, expected)\n",
    "    assert tuple(sentence) == (\"parse\", \"this\", \"sentence\"), \\\n",
    "        \"parse test failed: the input sentence should not be modified\"\n",
    "    print(\"parse test passed!\")\n",
    "\n",
    "\n",
    "class DummyModel(object):\n",
    "    \"\"\"Dummy model for testing the minibatch_parse function\n",
    "    First shifts everything onto the stack and then does exclusively right arcs if the first word of\n",
    "    the sentence is \"right\", \"left\" if otherwise.\n",
    "    \"\"\"\n",
    "    def predict(self, partial_parses):\n",
    "        return [(\"RA\" if pp.stack[1] is \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
    "                for pp in partial_parses]\n",
    "\n",
    "\n",
    "def test_dependencies(name, deps, ex_deps):\n",
    "    \"\"\"Tests the provided dependencies match the expected dependencies\"\"\"\n",
    "    deps = tuple(sorted(deps))\n",
    "    assert deps == ex_deps, \\\n",
    "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
    "\n",
    "\n",
    "def test_minibatch_parse():\n",
    "    \"\"\"Simple tests for the minibatch_parse function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    sentences = [[\"right\", \"arcs\", \"only\"],\n",
    "                 [\"right\", \"arcs\", \"only\", \"again\"],\n",
    "                 [\"left\", \"arcs\", \"only\"],\n",
    "                 [\"left\", \"arcs\", \"only\", \"again\"]]\n",
    "    deps = minibatch_parse(sentences, DummyModel(), 2)\n",
    "    test_dependencies(\"minibatch_parse\", deps[0],\n",
    "                      (('ROOT', 'right'), ('arcs', 'only'), ('right', 'arcs')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[1],\n",
    "                      (('ROOT', 'right'), ('arcs', 'only'), ('only', 'again'), ('right', 'arcs')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[2],\n",
    "                      (('only', 'ROOT'), ('only', 'arcs'), ('only', 'left')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[3],\n",
    "                      (('again', 'ROOT'), ('again', 'arcs'), ('again', 'left'), ('again', 'only')))\n",
    "    print(\"minibatch_parse test passed!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_parse_step()\n",
    "    test_parse()\n",
    "    test_minibatch_parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "Basic (non-exhaustive) Xavier initialization tests pass\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def xavier_weight_init():\n",
    "    \"\"\"Returns function that creates random tensor.\n",
    "\n",
    "    The specified function will take in a shape (tuple or 1-d array) and\n",
    "    returns a random tensor of the specified shape drawn from the\n",
    "    Xavier initialization distribution.\n",
    "\n",
    "    Hint: You might find tf.random_uniform useful.\n",
    "    \"\"\"\n",
    "    def _xavier_initializer(shape, **kwargs):\n",
    "        \"\"\"Defines an initializer for the Xavier distribution.\n",
    "        Specifically, the output should be sampled uniformly from [-epsilon, epsilon] where\n",
    "            epsilon = sqrt(6) / <sum of the sizes of shape's dimensions>\n",
    "        e.g., if shape = (2, 3), epsilon = sqrt(6 / (2 + 3))\n",
    "\n",
    "        This function will be used as a variable initializer.\n",
    "\n",
    "        Args:\n",
    "            shape: Tuple or 1-d array that species the dimensions of the requested tensor.\n",
    "        Returns:\n",
    "            out: tf.Tensor of specified shape sampled from the Xavier distribution.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        epsilon = tf.sqrt(6.0 / tf.cast(tf.reduce_sum(shape), tf.float32))\n",
    "        out = tf.random_uniform(shape, -epsilon, epsilon)\n",
    "        ### END YOUR CODE\n",
    "        return out\n",
    "    # Returns defined initializer function.\n",
    "    return _xavier_initializer\n",
    "\n",
    "\n",
    "def test_initialization_basic():\n",
    "    \"\"\"Some simple tests for the initialization.\n",
    "    \"\"\"\n",
    "    print (\"Running basic tests...\")\n",
    "    xavier_initializer = xavier_weight_init()\n",
    "    shape = (1,)\n",
    "    xavier_mat = xavier_initializer(shape)\n",
    "    assert xavier_mat.get_shape() == shape\n",
    "\n",
    "    shape = (1, 2, 3)\n",
    "    xavier_mat = xavier_initializer(shape)\n",
    "    assert xavier_mat.get_shape() == shape\n",
    "    print (\"Basic (non-exhaustive) Xavier initialization tests pass\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_initialization_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING\n",
      "================================================================================\n",
      "Loading data...\n",
      "took 3.49 seconds\n",
      "Building parser...\n",
      "took 0.07 seconds\n",
      "Loading pretrained embeddings...\n",
      "took 3.44 seconds\n",
      "Vectorizing data...\n",
      "took 0.12 seconds\n",
      "Preprocessing training data...\n",
      "took 2.91 seconds\n",
      "Building model...\n",
      "took 0.43 seconds\n",
      "\n",
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n",
      "47/48 [============================>.] - ETA: 0s - train loss: 0.6514Evaluating on dev set\n",
      "- dev UAS: 51.15\n",
      "\n",
      "Epoch 2 out of 10\n",
      "47/48 [============================>.] - ETA: 0s - train loss: 0.3628Evaluating on dev set\n",
      "- dev UAS: 57.54\n",
      "\n",
      "Epoch 3 out of 10\n",
      "47/48 [============================>.] - ETA: 0s - train loss: 0.2973Evaluating on dev set\n",
      "- dev UAS: 62.13\n",
      "\n",
      "Epoch 4 out of 10\n",
      "47/48 [============================>.] - ETA: 0s - train loss: 0.2560Evaluating on dev set\n",
      "- dev UAS: 64.65\n",
      "\n",
      "Epoch 5 out of 10\n",
      "47/48 [============================>.] - ETA: 0s - train loss: 0.2260Evaluating on dev set\n",
      "- dev UAS: 65.62\n",
      "\n",
      "Epoch 6 out of 10\n",
      "47/48 [============================>.] - ETA: 0s - train loss: 0.2072Evaluating on dev set\n",
      "- dev UAS: 66.98\n",
      "\n",
      "Epoch 7 out of 10\n",
      "47/48 [============================>.] - ETA: 0s - train loss: 0.1875Evaluating on dev set\n",
      "- dev UAS: 67.10\n",
      "\n",
      "Epoch 8 out of 10\n",
      "47/48 [============================>.] - ETA: 0s - train loss: 0.1731Evaluating on dev set\n",
      "- dev UAS: 67.80\n",
      "\n",
      "Epoch 9 out of 10\n",
      "47/48 [============================>.] - ETA: 0s - train loss: 0.1618Evaluating on dev set\n",
      "- dev UAS: 68.95\n",
      "\n",
      "Epoch 10 out of 10\n",
      "47/48 [============================>.] - ETA: 0s - train loss: 0.1498Evaluating on dev set\n",
      "- dev UAS: 69.99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle as cPickle\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from model import Model\n",
    "from q2_initialization import xavier_weight_init\n",
    "from utils.parser_utils import minibatches, load_and_preprocess_data\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "    The config class is used to store various hyperparameters and dataset\n",
    "    information parameters. Model objects are passed a Config() object at\n",
    "    instantiation. They can then call self.config.<hyperparameter_name> to\n",
    "    get the hyperparameter settings.\n",
    "    \"\"\"\n",
    "    n_features = 36\n",
    "    n_classes = 3\n",
    "    dropout = 0.5  # (p_drop in the handout)\n",
    "    embed_size = 50\n",
    "    hidden_size = 200\n",
    "    batch_size = 1024\n",
    "    n_epochs = 10\n",
    "    lr = 0.0005\n",
    "\n",
    "\n",
    "class ParserModel(Model):\n",
    "    \"\"\"\n",
    "    Implements a feedforward neural network with an embedding layer and single hidden layer.\n",
    "    This network will predict which transition should be applied to a given partial parse\n",
    "    configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Generates placeholder variables to represent the input tensors\n",
    "\n",
    "        These placeholders are used as inputs by the rest of the model building and will be fed\n",
    "        data during training.  Note that when \"None\" is in a placeholder's shape, it's flexible\n",
    "        (so we can use different batch sizes without rebuilding the model).\n",
    "\n",
    "        Adds following nodes to the computational graph\n",
    "\n",
    "        input_placeholder: Input placeholder tensor of  shape (None, n_features), type tf.int32\n",
    "        labels_placeholder: Labels placeholder tensor of shape (None, n_classes), type tf.float32\n",
    "        dropout_placeholder: Dropout value placeholder (scalar), type tf.float32\n",
    "\n",
    "        Add these placeholders to self as the instance variables\n",
    "            self.input_placeholder\n",
    "            self.labels_placeholder\n",
    "            self.dropout_placeholder\n",
    "\n",
    "        (Don't change the variable names)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        n_features = self.config.n_features\n",
    "        n_classes = self.config.n_classes\n",
    "        \n",
    "        self.input_placeholder = tf.placeholder(tf.int32, shape = (None, n_features))\n",
    "        self.labels_placeholder = tf.placeholder(tf.float32, shape = (None, n_classes))\n",
    "        self.dropout_placeholder = tf.placeholder(tf.float32)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def create_feed_dict(self, inputs_batch, labels_batch=None, dropout=0):\n",
    "        \"\"\"Creates the feed_dict for the dependency parser.\n",
    "\n",
    "        A feed_dict takes the form of:\n",
    "\n",
    "        feed_dict = {\n",
    "                <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "                ....\n",
    "        }\n",
    "\n",
    "\n",
    "        Hint: The keys for the feed_dict should be a subset of the placeholder\n",
    "                    tensors created in add_placeholders.\n",
    "        Hint: When an argument is None, don't add it to the feed_dict.\n",
    "\n",
    "        Args:\n",
    "            inputs_batch: A batch of input data.\n",
    "            labels_batch: A batch of label data.\n",
    "            dropout: The dropout rate.\n",
    "        Returns:\n",
    "            feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        feed_dict = {\n",
    "            self.input_placeholder: inputs_batch,\n",
    "            self.dropout_placeholder: dropout\n",
    "        }\n",
    "        \n",
    "        if labels_batch is not None:\n",
    "            feed_dict[self.labels_placeholder] = labels_batch\n",
    "            \n",
    "        ### END YOUR CODE\n",
    "        return feed_dict\n",
    "\n",
    "    def add_embedding(self):\n",
    "        \"\"\"Adds an embedding layer that maps from input tokens (integers) to vectors and then\n",
    "        concatenates those vectors:\n",
    "            - Creates a tf.Variable and initializes it with self.pretrained_embeddings.\n",
    "            - Uses the input_placeholder to index into the embeddings tensor, resulting in a\n",
    "              tensor of shape (None, n_features, embedding_size).\n",
    "            - Concatenates the embeddings by reshaping the embeddings tensor to shape\n",
    "              (None, n_features * embedding_size).\n",
    "\n",
    "        Hint: You might find tf.nn.embedding_lookup useful.\n",
    "        Hint: You can use tf.reshape to concatenate the vectors. See following link to understand\n",
    "            what -1 in a shape means.\n",
    "            https://www.tensorflow.org/api_docs/python/tf/reshape\n",
    "\n",
    "        Returns:\n",
    "            embeddings: tf.Tensor of shape (None, n_features*embed_size)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        n_features = self.config.n_features\n",
    "        embedding_size = self.config.embed_size\n",
    "        \n",
    "        vocabulary = tf.Variable(self.pretrained_embeddings)\n",
    "        embeddings = tf.nn.embedding_lookup(vocabulary, self.input_placeholder)\n",
    "        #Shape: (None, n_features, embedding_size)\n",
    "        #Need to do reshape/concatenate\n",
    "        embeddings = tf.reshape(embeddings, (-1, n_features * embedding_size))\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        return embeddings\n",
    "\n",
    "    def add_prediction_op(self):\n",
    "        \"\"\"Adds the 1-hidden-layer NN:\n",
    "            h = Relu(xW + b1)\n",
    "            h_drop = Dropout(h, dropout_rate)\n",
    "            pred = h_dropU + b2\n",
    "\n",
    "        Note that we are not applying a softmax to pred. The softmax will instead be done in\n",
    "        the add_loss_op function, which improves efficiency because we can use\n",
    "        tf.nn.softmax_cross_entropy_with_logits\n",
    "\n",
    "        Use the initializer from q2_initialization.py to initialize W and U (you can initialize b1\n",
    "        and b2 with zeros)\n",
    "\n",
    "        Hint: Note that tf.nn.dropout takes the keep probability (1 - p_drop) as an argument.\n",
    "              Therefore the keep probability should be set to the value of\n",
    "              (1 - self.dropout_placeholder)\n",
    "\n",
    "        Returns:\n",
    "            pred: tf.Tensor of shape (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.add_embedding()\n",
    "        ### YOUR CODE HERE\n",
    "        #initializer to initialize W and U\n",
    "        xavier_init = xavier_weight_init()\n",
    "        \n",
    "        n_features = self.config.n_features\n",
    "        n_classes = self.config.n_classes\n",
    "        embedding_size = self.config.embed_size\n",
    "        hidden_size = self.config.hidden_size\n",
    "        \n",
    "        #Hidden_layer\n",
    "        W = tf.Variable(xavier_init((n_features * embedding_size, hidden_size)))\n",
    "        b1 = tf.Variable(tf.zeros((1, hidden_size)))\n",
    "        z = tf.add(tf.matmul(x, W), b1)\n",
    "        h = tf.nn.relu(z)\n",
    "        \n",
    "        h_drop = tf.nn.dropout(h, keep_prob = 1 - self.dropout_placeholder)\n",
    "        \n",
    "        #Output_layer\n",
    "        U = tf.Variable(xavier_init((hidden_size, n_classes)))\n",
    "        b2 = tf.Variable(tf.zeros((1, n_classes)))\n",
    "        \n",
    "        pred = tf.add(tf.matmul(h_drop, U), b2)\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        return pred\n",
    "\n",
    "    def add_loss_op(self, pred):\n",
    "        \"\"\"Adds Ops for the loss function to the computational graph.\n",
    "        In this case we are using cross entropy loss.\n",
    "        The loss should be averaged over all examples in the current minibatch.\n",
    "\n",
    "        Hint: You can use tf.nn.softmax_cross_entropy_with_logits to simplify your\n",
    "                    implementation. You might find tf.reduce_mean useful.\n",
    "        Args:\n",
    "            pred: A tensor of shape (batch_size, n_classes) containing the output of the neural\n",
    "                  network before the softmax layer.\n",
    "        Returns:\n",
    "            loss: A 0-d tensor (scalar)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.labels_placeholder, \n",
    "                                                                      logits = pred))\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        return loss\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        `sess.run()` call to cause the model to train. See\n",
    "\n",
    "        https://www.tensorflow.org/api_docs/python/tf/train/Optimizer\n",
    "\n",
    "        for more information.\n",
    "\n",
    "        Use tf.train.AdamOptimizer for this model.\n",
    "        Use the learning rate from self.config.\n",
    "        Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "        Args:\n",
    "            loss: Loss tensor, from cross_entropy_loss.\n",
    "        Returns:\n",
    "            train_op: The Op for training.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        optimizer = tf.train.AdamOptimizer(self.config.lr)\n",
    "        train_op = optimizer.minimize(loss)\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        return train_op\n",
    "\n",
    "    def train_on_batch(self, sess, inputs_batch, labels_batch):\n",
    "        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch,\n",
    "                                     dropout=self.config.dropout)\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)\n",
    "        return loss\n",
    "\n",
    "    def run_epoch(self, sess, parser, train_examples, dev_set):\n",
    "        n_minibatches = 1 + len(train_examples) / self.config.batch_size\n",
    "        prog = tf.keras.utils.Progbar(target=n_minibatches)\n",
    "        for i, (train_x, train_y) in enumerate(minibatches(train_examples, self.config.batch_size)):\n",
    "            loss = self.train_on_batch(sess, train_x, train_y)\n",
    "            prog.update(i + 1, [(\"train loss\", loss)], force=i + 1 == n_minibatches)\n",
    "\n",
    "        print(\"  Evaluating on dev set\",)\n",
    "        dev_UAS, _ = parser.parse(dev_set)\n",
    "        print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
    "        return dev_UAS\n",
    "\n",
    "    def fit(self, sess, saver, parser, train_examples, dev_set):\n",
    "        best_dev_UAS = 0\n",
    "        for epoch in range(self.config.n_epochs):\n",
    "            print( \"Epoch {:} out of {:}\".format(epoch + 1, self.config.n_epochs))\n",
    "            dev_UAS = self.run_epoch(sess, parser, train_examples, dev_set)\n",
    "            if dev_UAS > best_dev_UAS:\n",
    "                best_dev_UAS = dev_UAS\n",
    "                if saver:\n",
    "                    print( \"New best dev UAS! Saving model in ./data/weights/parser.weights\")\n",
    "                    saver.save(sess, './data/weights/parser.weights')\n",
    "            print()\n",
    "\n",
    "    def __init__(self, config, pretrained_embeddings):\n",
    "        self.pretrained_embeddings = pretrained_embeddings\n",
    "        self.config = config\n",
    "        self.build()\n",
    "\n",
    "\n",
    "def main(debug=True):\n",
    "    print( 80 * \"=\")\n",
    "    print( \"INITIALIZING\")\n",
    "    print( 80 * \"=\")\n",
    "    config = Config()\n",
    "    parser, embeddings, train_examples, dev_set, test_set = load_and_preprocess_data(debug)\n",
    "    if not os.path.exists('./data/weights/'):\n",
    "        os.makedirs('./data/weights/')\n",
    "\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        print( \"Building model...\",)\n",
    "        start = time.time()\n",
    "        model = ParserModel(config, embeddings)\n",
    "        parser.model = model\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        saver = None if debug else tf.train.Saver()\n",
    "        print( \"took {:.2f} seconds\\n\".format(time.time() - start))\n",
    "    graph.finalize()\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        parser.session = session\n",
    "        session.run(init_op)\n",
    "\n",
    "        print( 80 * \"=\")\n",
    "        print( \"TRAINING\")\n",
    "        print( 80 * \"=\")\n",
    "        model.fit(session, saver, parser, train_examples, dev_set)\n",
    "\n",
    "        if not debug:\n",
    "            print( 80 * \"=\")\n",
    "            print( \"TESTING\")\n",
    "            print( 80 * \"=\")\n",
    "            print( \"Restoring the best model weights found on the dev set\")\n",
    "            saver.restore(session, './data/weights/parser.weights')\n",
    "            print( \"Final evaluation on test set\",)\n",
    "            UAS, dependencies = parser.parse(test_set)\n",
    "            print( \"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "            print( \"Writing predictions\")\n",
    "            with open('q2_test.predicted.pkl', 'w') as f:\n",
    "                cPickle.dump(dependencies, f, -1)\n",
    "            print( \"Done!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
