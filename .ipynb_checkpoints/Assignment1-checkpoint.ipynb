{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Softmax (10 points)\n",
    "\n",
    "(a) (5 points) Prove that softmax is invariant to constant offsets in the input, that is, for any input vector\n",
    "x and any constant c, softmax(x) = softmax(x + c)\n",
    "where x + c means adding the constant c to every dimension of x.\n",
    "\n",
    "Note: In practice, we make use of this property and choose c = âˆ’ maxi xi when computing softmax\n",
    "probabilities for numerical stability (i.e., subtracting its maximum element from all elements of x).\n",
    "\n",
    "Ans: Apply chain rule in calculus. http://web.stanford.edu/class/cs224n/assignment1/assignment1-solution.pdf\n",
    "\n",
    "(b) (5 points) Given an input matrix of N rows and D columns, compute the softmax prediction for each\n",
    "row using the optimization in part (a). Write your implementation in q1 softmax.py. You may test\n",
    "by executing python q1 softmax.py.\n",
    "Note: The provided tests are not exhaustive. Later parts of the assignment will reference this code so\n",
    "it is important to have a correct implementation. Your implementation should also be efficient and\n",
    "vectorized whenever possible (i.e., use numpy matrix operations rather than for loops). A non-vectorized\n",
    "implementation will not receive full credit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute the softmax function for each row of the input x.\n",
    "\n",
    "    It is crucial that this function is optimized for speed because\n",
    "    it will be used frequently in later code. You might find numpy\n",
    "    functions np.exp, np.sum, np.reshape, np.max, and numpy\n",
    "    broadcasting useful for this task.\n",
    "\n",
    "    Numpy broadcasting documentation:\n",
    "    http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n",
    "\n",
    "    You should also make sure that your code works for a single\n",
    "    D-dimensional vector (treat the vector as a single row) and\n",
    "    for N x D matrices. This may be useful for testing later. Also,\n",
    "    make sure that the dimensions of the output match the input.\n",
    "\n",
    "    You must implement the optimization in problem 1(a) of the\n",
    "    written assignment!\n",
    "\n",
    "    Arguments:\n",
    "    x -- A D dimensional vector or N x D dimensional numpy matrix.\n",
    "\n",
    "    Return:\n",
    "    x -- You are allowed to modify x in-place\n",
    "    \"\"\"\n",
    "    orig_shape = x.shape\n",
    "\n",
    "    if len(orig_shape) > 1:\n",
    "        # Matrix\n",
    "        ### YOUR CODE HERE\n",
    "        #compute exp for each row, avoid using for loop, and np.exp directly (or the number would explode)\n",
    "        #since np.exp(x) would output e^x\n",
    "        e = np.exp(x - x.max(axis = 1, keepdims = True))\n",
    "        s = np.sum(e, axis = 1, keepdims = True)\n",
    "        x = e / s\n",
    "        ### END YOUR CODE\n",
    "    else:\n",
    "        # Vector\n",
    "        ### YOUR CODE HERE\n",
    "        e = np.exp(x - x.max())\n",
    "        s = np.sum(e)\n",
    "        x = e / s\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    assert x.shape == orig_shape\n",
    "    return x\n",
    "\n",
    "\n",
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print (\"Running basic tests...\")\n",
    "    test1 = softmax(np.array([1,2]))\n",
    "    print (test1)\n",
    "    ans1 = np.array([0.26894142,  0.73105858])\n",
    "    assert np.allclose(test1, ans1, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "    test2 = softmax(np.array([[1001,1002],[3,4]]))\n",
    "    print (test2)\n",
    "    ans2 = np.array([\n",
    "        [0.26894142, 0.73105858],\n",
    "        [0.26894142, 0.73105858]])\n",
    "    assert np.allclose(test2, ans2, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "    test3 = softmax(np.array([[-1001,-1002]]))\n",
    "    print (test3)\n",
    "    ans3 = np.array([0.73105858, 0.26894142])\n",
    "    assert np.allclose(test3, ans3, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "    print (\"You should be able to verify these results by hand!\\n\")\n",
    "\n",
    "\n",
    "def test_softmax():\n",
    "    \"\"\"\n",
    "    Use this space to test your softmax implementation by running:\n",
    "        python q1_softmax.py\n",
    "    This function will not be called by the autograder, nor will\n",
    "    your tests be graded.\n",
    "    \"\"\"\n",
    "    print (\"Running your tests...\")\n",
    "    ### YOUR CODE HERE\n",
    "    #raise NotImplementedError\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[0.26894142 0.73105858]\n",
      "[[0.26894142 0.73105858]\n",
      " [0.26894142 0.73105858]]\n",
      "[[0.73105858 0.26894142]]\n",
      "You should be able to verify these results by hand!\n",
      "\n",
      "Running your tests...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_softmax_basic()\n",
    "    test_softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2-1](assignment1/2_1.png)\n",
    "<img src=\"assignment1/2_2.png\" style=\"width:1000px;height:650px;\">\n",
    "<caption><center> <u> **** </u> ****<br> </center></caption>\n",
    "![2-3](assignment1/2_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Derivative of sigmoid\n",
    "# dsigmoid(x) / dx = (1 / (1 + e^(-x)))^2 * e^(-x) * -1\n",
    "#                  = (1 / (1 + e^(-x)))(-e^(-x) / (1 + e^(-x)))\n",
    "#                  = sigmoid(x) * (1 - sigmoid(x))\n",
    "def de_sigmoid(x):\n",
    "    derivative = sigmoid(x) * (1 - sigmoid(x))\n",
    "    return derivative\n",
    "\n",
    "#Derivative of theta to the cross entropy loss\n",
    "# y^ = softmax(z) = e^z / sum(e^z)\n",
    "# ce(y, y^) = -sum(y * log(y^))             note: y be either 0 or 1\n",
    "#### d(g(x)/h(x)) / dx = d(g(x))/dx * 1/h(x) - g(x)/(h(x)^2) * d(h(x))/dx\n",
    "# https://stats.stackexchange.com/questions/235528/backpropagation-with-softmax-cross-entropy\n",
    "# https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function\n",
    "def de_softmax(y, y_hat):\n",
    "    return y_hat - y\n",
    "\n",
    "#Derivative of input to the cross entropy loss\n",
    "#See below implementation\n",
    "\n",
    "#How many params to be trained?\n",
    "#(Dx * H) + H +(H * Dy) + Dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[[0.73105858 0.88079708]\n",
      " [0.26894142 0.11920292]]\n",
      "[[0.19661193 0.10499359]\n",
      " [0.19661193 0.10499359]]\n",
      "You should verify these results by hand!\n",
      "\n",
      "Running your tests...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    ### END YOUR CODE\n",
    "    return s\n",
    "\n",
    "def sigmoid_grad(s):\n",
    "    \"\"\"\n",
    "    Compute the gradient for the sigmoid function here. Note that\n",
    "    for this implementation, the input s should be the sigmoid\n",
    "    function value of your original input x.\n",
    "\n",
    "    Arguments:\n",
    "    s -- A scalar or numpy array.\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    ds = s * (1 - s)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def test_sigmoid_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print (\"Running basic tests...\")\n",
    "    x = np.array([[1, 2], [-1, -2]])\n",
    "    f = sigmoid(x)\n",
    "    g = sigmoid_grad(f)\n",
    "    print (f)\n",
    "    f_ans = np.array([\n",
    "        [0.73105858, 0.88079708],\n",
    "        [0.26894142, 0.11920292]])\n",
    "    assert np.allclose(f, f_ans, rtol=1e-05, atol=1e-06)\n",
    "    print (g)\n",
    "    g_ans = np.array([\n",
    "        [0.19661193, 0.10499359],\n",
    "        [0.19661193, 0.10499359]])\n",
    "    assert np.allclose(g, g_ans, rtol=1e-05, atol=1e-06)\n",
    "    print (\"You should verify these results by hand!\\n\")\n",
    "\n",
    "\n",
    "def test_sigmoid():\n",
    "    \"\"\"\n",
    "    Use this space to test your sigmoid implementation by running:\n",
    "        python q2_sigmoid.py\n",
    "    This function will not be called by the autograder, nor will\n",
    "    your tests be graded.\n",
    "    \"\"\"\n",
    "    print (\"Running your tests...\")\n",
    "    ### YOUR CODE HERE\n",
    "    #raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_sigmoid_basic();\n",
    "    test_sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "Running your sanity checks...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# First implement a gradient checker by filling in the following functions\n",
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" Gradient check for a function f.\n",
    "\n",
    "    Arguments:\n",
    "    f -- a function that takes a single argument and outputs the\n",
    "         cost and its gradients\n",
    "    x -- the point (numpy array) to check the gradient at\n",
    "    \"\"\"\n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)\n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4        # Do not change this!\n",
    "\n",
    "    # Iterate over all indexes ix in x to check the gradient.\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        # Try modifying x[ix] with h defined above to compute numerical\n",
    "        # gradients (numgrad).\n",
    "\n",
    "        # Use the centered difference of the gradient.\n",
    "        # It has smaller asymptotic error than forward / backward difference\n",
    "        # methods. If you are curious, check out here:\n",
    "        # https://math.stackexchange.com/questions/2326181/when-to-use-forward-or-central-difference-approximations\n",
    "\n",
    "        # Make sure you call random.setstate(rndstate)\n",
    "        # before calling f(x) each time. This will make it possible\n",
    "        # to test cost functions with built in randomness later.\n",
    "\n",
    "        ### YOUR CODE HERE:\n",
    "        x[ix] += h\n",
    "\n",
    "        random.setstate(rndstate)\n",
    "        new_f1, _ = f(x)\n",
    "\n",
    "        x[ix] -= 2*h\n",
    "\n",
    "        random.setstate(rndstate)\n",
    "        new_f2, _ = f(x)\n",
    "        \n",
    "        x[ix] += h\n",
    "\n",
    "        numgrad = (new_f1 - new_f2) / (2 * h)\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print (\"Gradient check failed.\")\n",
    "            print (\"First gradient error found at index %s\" % str(ix))\n",
    "            print (\"Your gradient: %f \\t Numerical gradient: %f\" % (\n",
    "                grad[ix], numgrad))\n",
    "            return\n",
    "\n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print (\"Gradient check passed!\")\n",
    "\n",
    "\n",
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Some basic sanity checks.\n",
    "    \"\"\"\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print (\"Running sanity checks...\")\n",
    "    gradcheck_naive(quad, np.array(123.456))      # scalar test\n",
    "    gradcheck_naive(quad, np.random.randn(3,))    # 1-D test\n",
    "    gradcheck_naive(quad, np.random.randn(4,5))   # 2-D test\n",
    "    print (\"\")\n",
    "\n",
    "\n",
    "def your_sanity_checks():\n",
    "    \"\"\"\n",
    "    Use this space add any additional sanity checks by running:\n",
    "        python q2_gradcheck.py\n",
    "    This function will not be called by the autograder, nor will\n",
    "    your additional tests be graded.\n",
    "    \"\"\"\n",
    "    print (\"Running your sanity checks...\")\n",
    "    ### YOUR CODE HERE\n",
    "    #raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sanity_check()\n",
    "    your_sanity_checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity check...\n",
      "Gradient check failed.\n",
      "First gradient error found at index (0,)\n",
      "Your gradient: -1.693877 \t Numerical gradient: 0.455516\n",
      "Running your sanity checks...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#from q1_softmax import softmax\n",
    "#from q2_sigmoid import sigmoid, sigmoid_grad\n",
    "#from q2_gradcheck import gradcheck_naive\n",
    "\n",
    "\n",
    "def forward_backward_prop(X, labels, params, dimensions):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation for a two-layer sigmoidal network\n",
    "\n",
    "    Compute the forward propagation and for the cross entropy cost,\n",
    "    the backward propagation for the gradients for all parameters.\n",
    "\n",
    "    Notice the gradients computed here are different from the gradients in\n",
    "    the assignment sheet: they are w.r.t. weights, not inputs.\n",
    "\n",
    "    Arguments:\n",
    "    X -- M x Dx matrix, where each row is a training example x.\n",
    "    labels -- M x Dy matrix, where each row is a one-hot vector.\n",
    "    params -- Model parameters, these are unpacked for you.\n",
    "    dimensions -- A tuple of input dimension, number of hidden units\n",
    "                  and output dimension\n",
    "    \"\"\"\n",
    "\n",
    "    ### Unpack network parameters (do not modify)\n",
    "    ofs = 0\n",
    "    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
    "\n",
    "    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H)) #(10, 5)\n",
    "    ofs += Dx * H\n",
    "    b1 = np.reshape(params[ofs:ofs + H], (1, H)) #(1,5)\n",
    "    ofs += H\n",
    "    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy)) #(5, 10)\n",
    "    ofs += H * Dy\n",
    "    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n",
    "\n",
    "    # Note: compute cost based on `sum` not `mean`.\n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    Z1 = X.dot(W1) + b1\n",
    "    A1 = sigmoid(Z1) #(20,5)\n",
    "    Z2 = A1.dot(W2) + b2 #(20,10)\n",
    "    y_hat = softmax(Z2)\n",
    "    \n",
    "    cost = -np.sum(labels*np.log(y_hat))\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "    gradZ2 = 1./len(X) * (y_hat - labels) #(20,10)\n",
    "    gradW2 = np.dot(A1.T, gradZ2) #(5, 10)\n",
    "    gradb2 = np.sum(gradZ2, axis = 0, keepdims = True)\n",
    "    gradA1 = np.dot(gradZ2, W2.T) #(20,5)\n",
    "    gradZ1 = gradA1 * Z1 * (1-Z1) #(20,5)\n",
    "    gradW1 = np.dot(X.T, gradZ1) #(10,5)\n",
    "    gradb1 = np.sum(gradZ1, axis = 0, keepdims = True)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(),\n",
    "        gradW2.flatten(), gradb2.flatten()))\n",
    "\n",
    "    return cost, grad\n",
    "\n",
    "\n",
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Set up fake data and parameters for the neural network, and test using\n",
    "    gradcheck.\n",
    "    \"\"\"\n",
    "    print (\"Running sanity check...\")\n",
    "\n",
    "    N = 20\n",
    "    dimensions = [10, 5, 10]\n",
    "    data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "    labels = np.zeros((N, dimensions[2]))\n",
    "    for i in range(N):\n",
    "        labels[i, random.randint(0,dimensions[2]-1)] = 1\n",
    "\n",
    "    params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (\n",
    "        dimensions[1] + 1) * dimensions[2], )\n",
    "\n",
    "    gradcheck_naive(lambda params:\n",
    "        forward_backward_prop(data, labels, params, dimensions), params)\n",
    "\n",
    "\n",
    "def your_sanity_checks():\n",
    "    \"\"\"\n",
    "    Use this space add any additional sanity checks by running:\n",
    "        python q2_neural.py\n",
    "    This function will not be called by the autograder, nor will\n",
    "    your additional tests be graded.\n",
    "    \"\"\"\n",
    "    print (\"Running your sanity checks...\")\n",
    "    ### YOUR CODE HERE\n",
    "    #raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sanity_check()\n",
    "    your_sanity_checks()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. word2vec\n",
    "See details: http://web.stanford.edu/class/cs224n/assignment1/assignment1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grad word_i of cost function:\n",
    "#for word t in sentence X, the gradient of the wordvec to the softmax function is:\n",
    "#grad_wordvec_t = Y[t] - sum(softmax(X[t])*wordvec[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing normalizeRows...\n",
      "[[0.6        0.8       ]\n",
      " [0.4472136  0.89442719]]\n",
      "\n",
      "==== Gradient check for skip-gram ====\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "==== Gradient check for CBOW      ====\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "=== Results ===\n",
      "(11.16610900153398, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-1.26947339, -1.36873189,  2.45158957],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.41045956,  0.18834851,  1.43272264],\n",
      "       [ 0.38202831, -0.17530219, -1.33348241],\n",
      "       [ 0.07009355, -0.03216399, -0.24466386],\n",
      "       [ 0.09472154, -0.04346509, -0.33062865],\n",
      "       [-0.13638384,  0.06258276,  0.47605228]]))\n",
      "(14.093692760899629, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-3.86802836, -1.12713967, -1.52668625],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.11265089,  0.05169237,  0.39321163],\n",
      "       [-0.22716495,  0.10423969,  0.79292674],\n",
      "       [-0.79674766,  0.36560539,  2.78107395],\n",
      "       [-0.31602611,  0.14501561,  1.10309954],\n",
      "       [-0.80620296,  0.36994417,  2.81407799]]))\n",
      "(0.798995801090665, array([[ 0.23330542, -0.51643128, -0.8281311 ],\n",
      "       [ 0.11665271, -0.25821564, -0.41406555],\n",
      "       [ 0.11665271, -0.25821564, -0.41406555],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 0.80954933,  0.21962514, -0.54095764],\n",
      "       [-0.03556575, -0.00964874,  0.02376577],\n",
      "       [-0.13016109, -0.0353118 ,  0.08697634],\n",
      "       [-0.1650812 , -0.04478539,  0.11031068],\n",
      "       [-0.47874129, -0.1298792 ,  0.31990485]]))\n",
      "(7.895593203599139, array([[-2.98873309, -3.38440688, -2.62676289],\n",
      "       [-1.49436655, -1.69220344, -1.31338145],\n",
      "       [-1.49436655, -1.69220344, -1.31338145],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 0.21992784,  0.0596649 , -0.14696034],\n",
      "       [-1.37825047, -0.37390982,  0.92097553],\n",
      "       [-0.77702167, -0.21080061,  0.51922198],\n",
      "       [-2.58955401, -0.7025281 ,  1.73039366],\n",
      "       [-2.36749007, -0.64228369,  1.58200593]]))\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/6392739/what-does-the-at-symbol-do-in-python\n",
    "#In python3.5 you can overload @ as an operator. It is named as __matmul__ because \n",
    "#It is designed to do matrix multiplication, but It can be anything you want. see PEP465 for details.\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#from q1_softmax import softmax\n",
    "#from q2_gradcheck import gradcheck_naive\n",
    "#from q2_sigmoid import sigmoid, sigmoid_grad\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\" \n",
    "    Row normalization function\n",
    "\n",
    "    Implement a function that normalizes each row of a matrix to have\n",
    "    unit length.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    #power_x = np.power(x, 2)\n",
    "    #ps_x = power_x.sum(axis = 1).reshape(x.shape[0],-1)\n",
    "    #x /= np.sqrt(ps_x)\n",
    "    norm=np.sqrt(np.sum(x**2,1)).reshape(-1,1)\n",
    "    x /= norm\n",
    "    #raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def test_normalize_rows():\n",
    "    print (\"Testing normalizeRows...\")\n",
    "    x = normalizeRows(np.array([[3.0,4.0],[1, 2]]))\n",
    "    print (x)\n",
    "    ans = np.array([[0.6,0.8],[0.4472136,0.89442719]])\n",
    "    assert np.allclose(x, ans, rtol=1e-05, atol=1e-06)\n",
    "    print (\"\")\n",
    "\n",
    "\n",
    "def softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n",
    "    \"\"\" Softmax cost function for word2vec models\n",
    "\n",
    "    Implement the cost and gradients for one predicted word vector\n",
    "    and one target word vector as a building block for word2vec\n",
    "    models, assuming the softmax prediction function and cross\n",
    "    entropy loss.\n",
    "\n",
    "    Arguments:\n",
    "    predicted -- numpy ndarray, predicted word vector (\\hat{v} in\n",
    "                 the written component)\n",
    "    target -- integer, the index of the target word\n",
    "    outputVectors -- \"output\" vectors (as rows) for all tokens\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "\n",
    "    Return:\n",
    "    cost -- cross entropy cost for the softmax word prediction\n",
    "    gradPred -- the gradient with respect to the predicted word\n",
    "           vector\n",
    "    grad -- the gradient with respect to all the other word\n",
    "           vectors\n",
    "\n",
    "    We will not provide starter code for this function, but feel\n",
    "    free to reference the code you previously wrote for this\n",
    "    assignment!\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    probs = softmax( predicted.dot(outputVectors.T) )\n",
    "    cost = -np.log(probs[target])\n",
    "\n",
    "    grad_pred = probs\n",
    "    grad_pred[target] -= 1\n",
    "\n",
    "    grad = grad_pred[:, np.newaxis] * predicted[np.newaxis, :]\n",
    "    gradPred = grad_pred.dot(outputVectors)\n",
    "\n",
    "    #raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradPred, grad\n",
    "\n",
    "\n",
    "def getNegativeSamples(target, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the target \"\"\"\n",
    "\n",
    "    indices = [None] * K\n",
    "    for k in range(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == target:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        indices[k] = newidx\n",
    "    return indices\n",
    "\n",
    "\n",
    "def negSamplingCostAndGradient(predicted, target, outputVectors, dataset,\n",
    "                               K=10):\n",
    "    \"\"\" Negative sampling cost function for word2vec models\n",
    "\n",
    "    Implement the cost and gradients for one predicted word vector\n",
    "    and one target word vector as a building block for word2vec\n",
    "    models, using the negative sampling technique. K is the sample\n",
    "    size.\n",
    "\n",
    "    Note: See test_word2vec below for dataset's initialization.\n",
    "\n",
    "    Arguments/Return Specifications: same as softmaxCostAndGradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Sampling of indices is done for you. Do not modify this if you\n",
    "    # wish to match the autograder and receive points!\n",
    "    indices = [target]\n",
    "    indices.extend(getNegativeSamples(target, dataset, K))\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    # Probabilities of target and negative samples conditioned on center word\n",
    "    logits = outputVectors[indices].dot(predicted)\n",
    "    neg = np.full(logits.shape, -1)\n",
    "    neg[0] = 1\n",
    "    probs = sigmoid(logits * neg)\n",
    "\n",
    "    # Cost\n",
    "    cost = -np.log(probs).sum()\n",
    "\n",
    "    # Gradient wrt to logits\n",
    "    dlogits = neg * (probs - 1.)\n",
    "\n",
    "    # Gradient wrt to center word\n",
    "    gradPred = outputVectors[indices].T.dot(dlogits)\n",
    "\n",
    "    # Gradient wrt to outputVectors, need to sum where the same index occurs\n",
    "    # multiple times in the negative sampling\n",
    "    grad = np.zeros_like(outputVectors)\n",
    "    gradK = np.outer(dlogits, predicted)\n",
    "    for i, k in enumerate(indices):\n",
    "        grad[k] += gradK[i]\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradPred, grad\n",
    "\n",
    "\n",
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors,\n",
    "             dataset, word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec\n",
    "\n",
    "    Implement the skip-gram model in this function.\n",
    "\n",
    "    Arguments:\n",
    "    currentWord -- a string of the current center word\n",
    "    C -- integer, context size\n",
    "    contextWords -- list of no more than 2*C strings, the context words\n",
    "    tokens -- a dictionary that maps words to their indices in\n",
    "              the word vector list\n",
    "    inputVectors -- \"input\" word vectors (as rows) for all tokens\n",
    "    outputVectors -- \"output\" word vectors (as rows) for all tokens\n",
    "    word2vecCostAndGradient -- the cost and gradient function for\n",
    "                               a prediction vector given the target\n",
    "                               word vectors, could be one of the two\n",
    "                               cost functions you implemented above.\n",
    "\n",
    "    Return:\n",
    "    cost -- the cost function value for the skip-gram model\n",
    "    grad -- the gradient with respect to the word vectors\n",
    "    \"\"\"\n",
    "\n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    inputVector = inputVectors[tokens[currentWord]]\n",
    "\n",
    "    for contextWord in contextWords:\n",
    "        target = tokens[contextWord]\n",
    "        c, gradPred, grad = word2vecCostAndGradient(inputVector, target, outputVectors, dataset)\n",
    "        gradIn[tokens[currentWord]] += gradPred\n",
    "        gradOut += grad\n",
    "        cost += c\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "\n",
    "def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors,\n",
    "         dataset, word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    \"\"\"CBOW model in word2vec\n",
    "\n",
    "    Implement the continuous bag-of-words model in this function.\n",
    "\n",
    "    Arguments/Return specifications: same as the skip-gram model\n",
    "\n",
    "    Extra credit: Implementing CBOW is optional, but the gradient\n",
    "    derivations are not. If you decide not to implement CBOW, remove\n",
    "    the NotImplementedError.\n",
    "    \"\"\"\n",
    "\n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    # get integer of the center word (the word we want to predict)\n",
    "    target = tokens[currentWord]\n",
    "\n",
    "    # context_w correspond to the \\hat{v} vector\n",
    "    context_w = sum(inputVectors[tokens[w]] for w in contextWords)\n",
    "\n",
    "    cost, gradPred, gradOut = word2vecCostAndGradient(context_w, target, outputVectors, dataset)\n",
    "\n",
    "    gradIn = np.zeros_like(inputVectors)\n",
    "    \n",
    "    for w in contextWords:\n",
    "        gradIn[tokens[w]] += gradPred\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Testing functions below. DO NOT MODIFY!   #\n",
    "#############################################\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C,\n",
    "                         word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    batchsize = 50\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    inputVectors = wordVectors[:N//2,:]\n",
    "    outputVectors = wordVectors[N//2:,:]\n",
    "    for i in range(batchsize):\n",
    "        C1 = random.randint(1,C)\n",
    "        centerword, context = dataset.getRandomContext(C1)\n",
    "\n",
    "        if word2vecModel == skipgram:\n",
    "            denom = 1\n",
    "        else:\n",
    "            denom = 1\n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerword, C1, context, tokens, inputVectors, outputVectors,\n",
    "            dataset, word2vecCostAndGradient)\n",
    "        cost += c / batchsize / denom\n",
    "        grad[:N//2, :] += gin / batchsize / denom\n",
    "        grad[N//2:, :] += gout / batchsize / denom\n",
    "\n",
    "    return cost, grad\n",
    "\n",
    "\n",
    "def test_word2vec():\n",
    "    \"\"\" Interface to the dataset for negative sampling \"\"\"\n",
    "    dataset = type('dummy', (), {})()\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], \\\n",
    "            [tokens[random.randint(0,4)] for i in range(2*C)]\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "    print (\"==== Gradient check for skip-gram ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n",
    "        dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
    "        dummy_vectors)\n",
    "    print (\"\\n==== Gradient check for CBOW      ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        cbow, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n",
    "        dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
    "        dummy_vectors)\n",
    "\n",
    "    print (\"\\n=== Results ===\")\n",
    "    print (skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset))\n",
    "    print (skipgram(\"c\", 1, [\"a\", \"b\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset,\n",
    "        negSamplingCostAndGradient))\n",
    "    print (cbow(\"a\", 2, [\"a\", \"b\", \"c\", \"a\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset))\n",
    "    print (cbow(\"a\", 2, [\"a\", \"b\", \"a\", \"c\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset,\n",
    "        negSamplingCostAndGradient))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_normalize_rows()\n",
    "    test_word2vec()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "iter 100: 0.004578\n",
      "iter 200: 0.004353\n",
      "iter 300: 0.004136\n",
      "iter 400: 0.003929\n",
      "iter 500: 0.003733\n",
      "iter 600: 0.003546\n",
      "iter 700: 0.003369\n",
      "iter 800: 0.003200\n",
      "iter 900: 0.003040\n",
      "iter 1000: 0.002888\n",
      "test 1 result: 8.414836786079764e-10\n",
      "iter 100: 0.000000\n",
      "iter 200: 0.000000\n",
      "iter 300: 0.000000\n",
      "iter 400: 0.000000\n",
      "iter 500: 0.000000\n",
      "iter 600: 0.000000\n",
      "iter 700: 0.000000\n",
      "iter 800: 0.000000\n",
      "iter 900: 0.000000\n",
      "iter 1000: 0.000000\n",
      "test 2 result: 0.0\n",
      "iter 100: 0.041205\n",
      "iter 200: 0.039181\n",
      "iter 300: 0.037222\n",
      "iter 400: 0.035361\n",
      "iter 500: 0.033593\n",
      "iter 600: 0.031913\n",
      "iter 700: 0.030318\n",
      "iter 800: 0.028802\n",
      "iter 900: 0.027362\n",
      "iter 1000: 0.025994\n",
      "test 3 result: -2.524451035823933e-09\n",
      "\n",
      "Running your sanity checks...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Save parameters every a few SGD iterations as fail-safe\n",
    "SAVE_PARAMS_EVERY = 5000\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "import pickle\n",
    "\n",
    "\n",
    "def load_saved_params():\n",
    "    \"\"\"\n",
    "    A helper function that loads previously saved parameters and resets\n",
    "    iteration start.\n",
    "    \"\"\"\n",
    "    st = 0\n",
    "    for f in glob.glob(\"saved_params_*.npy\"):\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st):\n",
    "            st = iter\n",
    "\n",
    "    if st > 0:\n",
    "        with open(\"saved_params_%d.npy\" % st, \"rb\") as f:\n",
    "            params = pickle.load(f)\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None\n",
    "\n",
    "\n",
    "def save_params(iter, params):\n",
    "    with open(\"saved_params_%d.npy\" % iter, \"wb\") as f:\n",
    "        pickle.dump(params, f)\n",
    "        pickle.dump(random.getstate(), f)\n",
    "\n",
    "\n",
    "def sgd(f, x0, step, iterations, postprocessing=None, useSaved=False,\n",
    "        PRINT_EVERY=10):\n",
    "    \"\"\" Stochastic Gradient Descent\n",
    "\n",
    "    Implement the stochastic gradient descent method in this function.\n",
    "\n",
    "    Arguments:\n",
    "    f -- the function to optimize, it should take a single\n",
    "         argument and yield two outputs, a cost and the gradient\n",
    "         with respect to the arguments\n",
    "    x0 -- the initial point to start SGD from\n",
    "    step -- the step size for SGD\n",
    "    iterations -- total iterations to run SGD for\n",
    "    postprocessing -- postprocessing function for the parameters\n",
    "                      if necessary. In the case of word2vec we will need to\n",
    "                      normalize the word vectors to have unit length.\n",
    "    PRINT_EVERY -- specifies how many iterations to output loss\n",
    "\n",
    "    Return:\n",
    "    x -- the parameter value after SGD finishes\n",
    "    \"\"\"\n",
    "\n",
    "    # Anneal learning rate every several iterations\n",
    "    ANNEAL_EVERY = 20000\n",
    "\n",
    "    if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "\n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "\n",
    "    x = x0\n",
    "\n",
    "    if not postprocessing:\n",
    "        postprocessing = lambda x: x\n",
    "\n",
    "    expcost = None\n",
    "\n",
    "    for iter in range(start_iter + 1, iterations + 1):\n",
    "        # Don't forget to apply the postprocessing after every iteration!\n",
    "        # You might want to print the progress every few iterations.\n",
    "\n",
    "        cost = None\n",
    "        ### YOUR CODE HERE\n",
    "        cost, gradX = f(x)\n",
    "        \n",
    "        x -= step * gradX\n",
    "        if postprocessing:\n",
    "            x = postprocessing(x)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            if not expcost:\n",
    "                expcost = cost\n",
    "            else:\n",
    "                expcost = .95 * expcost + .05 * cost\n",
    "            print (\"iter %d: %f\" % (iter, expcost))\n",
    "\n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "            save_params(iter, x)\n",
    "\n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def sanity_check():\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print (\"Running sanity checks...\")\n",
    "    t1 = sgd(quad, 0.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print (\"test 1 result:\", t1)\n",
    "    assert abs(t1) <= 1e-6\n",
    "\n",
    "    t2 = sgd(quad, 0.0, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print (\"test 2 result:\", t2)\n",
    "    assert abs(t2) <= 1e-6\n",
    "\n",
    "    t3 = sgd(quad, -1.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print (\"test 3 result:\", t3)\n",
    "    assert abs(t3) <= 1e-6\n",
    "\n",
    "    print (\"\")\n",
    "\n",
    "\n",
    "def your_sanity_checks():\n",
    "    \"\"\"\n",
    "    Use this space add any additional sanity checks by running:\n",
    "        python q3_sgd.py\n",
    "    This function will not be called by the autograder, nor will\n",
    "    your additional tests be graded.\n",
    "    \"\"\"\n",
    "    print (\"Running your sanity checks...\")\n",
    "    ### YOUR CODE HERE\n",
    "    #raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sanity_check()\n",
    "    your_sanity_checks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "class StanfordSentiment:\n",
    "    def __init__(self, path=None, tablesize = 1000000):\n",
    "        if not path:\n",
    "            path = \"/Users/KunWuYao/utils/datasets/stanfordSentimentTreebank\"\n",
    "\n",
    "        self.path = path\n",
    "        self.tablesize = tablesize\n",
    "\n",
    "    def tokens(self):\n",
    "        if hasattr(self, \"_tokens\") and self._tokens:\n",
    "            return self._tokens\n",
    "\n",
    "        tokens = dict()\n",
    "        tokenfreq = dict()\n",
    "        wordcount = 0\n",
    "        revtokens = []\n",
    "        idx = 0\n",
    "\n",
    "        for sentence in self.sentences():\n",
    "            for w in sentence:\n",
    "                wordcount += 1\n",
    "                if not w in tokens:\n",
    "                    tokens[w] = idx\n",
    "                    revtokens += [w]\n",
    "                    tokenfreq[w] = 1\n",
    "                    idx += 1\n",
    "                else:\n",
    "                    tokenfreq[w] += 1\n",
    "\n",
    "        tokens[\"UNK\"] = idx\n",
    "        revtokens += [\"UNK\"]\n",
    "        tokenfreq[\"UNK\"] = 1\n",
    "        wordcount += 1\n",
    "\n",
    "        self._tokens = tokens\n",
    "        self._tokenfreq = tokenfreq\n",
    "        self._wordcount = wordcount\n",
    "        self._revtokens = revtokens\n",
    "        return self._tokens\n",
    "\n",
    "    def sentences(self):\n",
    "        if hasattr(self, \"_sentences\") and self._sentences:\n",
    "            return self._sentences\n",
    "\n",
    "        sentences = []\n",
    "        with open(self.path + \"/datasetSentences.txt\", \"r\") as f:\n",
    "            first = True\n",
    "            for line in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "\n",
    "                splitted = line.strip().split()[1:]\n",
    "                # Deal with some peculiar encoding issues with this file\n",
    "                sentences += [[w.lower() for w in splitted]]\n",
    "\n",
    "        self._sentences = sentences\n",
    "        self._sentlengths = np.array([len(s) for s in sentences])\n",
    "        self._cumsentlen = np.cumsum(self._sentlengths)\n",
    "\n",
    "        return self._sentences\n",
    "\n",
    "    def numSentences(self):\n",
    "        if hasattr(self, \"_numSentences\") and self._numSentences:\n",
    "            return self._numSentences\n",
    "        else:\n",
    "            self._numSentences = len(self.sentences())\n",
    "            return self._numSentences\n",
    "\n",
    "    def allSentences(self):\n",
    "        if hasattr(self, \"_allsentences\") and self._allsentences:\n",
    "            return self._allsentences\n",
    "\n",
    "        sentences = self.sentences()\n",
    "        rejectProb = self.rejectProb()\n",
    "        tokens = self.tokens()\n",
    "        allsentences = [[w for w in s\n",
    "            if 0 >= rejectProb[tokens[w]] or random.random() >= rejectProb[tokens[w]]]\n",
    "            for s in sentences * 30]\n",
    "\n",
    "        allsentences = [s for s in allsentences if len(s) > 1]\n",
    "\n",
    "        self._allsentences = allsentences\n",
    "\n",
    "        return self._allsentences\n",
    "\n",
    "    def getRandomContext(self, C=5):\n",
    "        allsent = self.allSentences()\n",
    "        sentID = random.randint(0, len(allsent) - 1)\n",
    "        sent = allsent[sentID]\n",
    "        wordID = random.randint(0, len(sent) - 1)\n",
    "\n",
    "        context = sent[max(0, wordID - C):wordID]\n",
    "        if wordID+1 < len(sent):\n",
    "            context += sent[wordID+1:min(len(sent), wordID + C + 1)]\n",
    "\n",
    "        centerword = sent[wordID]\n",
    "        context = [w for w in context if w != centerword]\n",
    "\n",
    "        if len(context) > 0:\n",
    "            return centerword, context\n",
    "        else:\n",
    "            return self.getRandomContext(C)\n",
    "\n",
    "    def sent_labels(self):\n",
    "        if hasattr(self, \"_sent_labels\") and self._sent_labels:\n",
    "            return self._sent_labels\n",
    "\n",
    "        dictionary = dict()\n",
    "        phrases = 0\n",
    "        with open(self.path + \"/dictionary.txt\", \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                splitted = line.split(\"|\")\n",
    "                dictionary[splitted[0].lower()] = int(splitted[1])\n",
    "                phrases += 1\n",
    "\n",
    "        labels = [0.0] * phrases\n",
    "        with open(self.path + \"/sentiment_labels.txt\", \"r\") as f:\n",
    "            first = True\n",
    "            for line in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                splitted = line.split(\"|\")\n",
    "                labels[int(splitted[0])] = float(splitted[1])\n",
    "\n",
    "        sent_labels = [0.0] * self.numSentences()\n",
    "        sentences = self.sentences()\n",
    "        for i in range(self.numSentences()):\n",
    "            sentence = sentences[i]\n",
    "            full_sent = \" \".join(sentence).replace('-lrb-', '(').replace('-rrb-', ')')\n",
    "            sent_labels[i] = labels[dictionary[full_sent]]\n",
    "\n",
    "        self._sent_labels = sent_labels\n",
    "        return self._sent_labels\n",
    "\n",
    "    def dataset_split(self):\n",
    "        if hasattr(self, \"_split\") and self._split:\n",
    "            return self._split\n",
    "\n",
    "        split = [[] for i in range(3)]\n",
    "        with open(self.path + \"/datasetSplit.txt\", \"r\") as f:\n",
    "            first = True\n",
    "            for line in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "\n",
    "                splitted = line.strip().split(\",\")\n",
    "                split[int(splitted[1]) - 1] += [int(splitted[0]) - 1]\n",
    "\n",
    "        self._split = split\n",
    "        return self._split\n",
    "\n",
    "    def getRandomTrainSentence(self):\n",
    "        split = self.dataset_split()\n",
    "        sentId = split[0][random.randint(0, len(split[0]) - 1)]\n",
    "        return self.sentences()[sentId], self.categorify(self.sent_labels()[sentId])\n",
    "\n",
    "    def categorify(self, label):\n",
    "        if label <= 0.2:\n",
    "            return 0\n",
    "        elif label <= 0.4:\n",
    "            return 1\n",
    "        elif label <= 0.6:\n",
    "            return 2\n",
    "        elif label <= 0.8:\n",
    "            return 3\n",
    "        else:\n",
    "            return 4\n",
    "\n",
    "    def getDevSentences(self):\n",
    "        return self.getSplitSentences(2)\n",
    "\n",
    "    def getTestSentences(self):\n",
    "        return self.getSplitSentences(1)\n",
    "\n",
    "    def getTrainSentences(self):\n",
    "        return self.getSplitSentences(0)\n",
    "\n",
    "    def getSplitSentences(self, split=0):\n",
    "        ds_split = self.dataset_split()\n",
    "        return [(self.sentences()[i], self.categorify(self.sent_labels()[i])) for i in ds_split[split]]\n",
    "\n",
    "    def sampleTable(self):\n",
    "        if hasattr(self, '_sampleTable') and self._sampleTable is not None:\n",
    "            return self._sampleTable\n",
    "\n",
    "        nTokens = len(self.tokens())\n",
    "        samplingFreq = np.zeros((nTokens,))\n",
    "        self.allSentences()\n",
    "        i = 0\n",
    "        for w in range(nTokens):\n",
    "            w = self._revtokens[i]\n",
    "            if w in self._tokenfreq:\n",
    "                freq = 1.0 * self._tokenfreq[w]\n",
    "                # Reweigh\n",
    "                freq = freq ** 0.75\n",
    "            else:\n",
    "                freq = 0.0\n",
    "            samplingFreq[i] = freq\n",
    "            i += 1\n",
    "\n",
    "        samplingFreq /= np.sum(samplingFreq)\n",
    "        samplingFreq = np.cumsum(samplingFreq) * self.tablesize\n",
    "\n",
    "        self._sampleTable = [0] * self.tablesize\n",
    "\n",
    "        j = 0\n",
    "        for i in range(self.tablesize):\n",
    "            while i > samplingFreq[j]:\n",
    "                j += 1\n",
    "            self._sampleTable[i] = j\n",
    "\n",
    "        return self._sampleTable\n",
    "\n",
    "    def rejectProb(self):\n",
    "        if hasattr(self, '_rejectProb') and self._rejectProb is not None:\n",
    "            return self._rejectProb\n",
    "\n",
    "        threshold = 1e-5 * self._wordcount\n",
    "\n",
    "        nTokens = len(self.tokens())\n",
    "        rejectProb = np.zeros((nTokens,))\n",
    "        for i in range(nTokens):\n",
    "            w = self._revtokens[i]\n",
    "            freq = 1.0 * self._tokenfreq[w]\n",
    "            # Reweigh\n",
    "            rejectProb[i] = max(0, 1 - np.sqrt(threshold / freq))\n",
    "\n",
    "        self._rejectProb = rejectProb\n",
    "        return self._rejectProb\n",
    "\n",
    "    def sampleTokenIdx(self):\n",
    "        return self.sampleTable()[random.randint(0, self.tablesize - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanity check: cost at convergence should be around or below 10\n",
      "training took 0 seconds\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "#from utils.treebank import StanfordSentiment\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#from q3_word2vec import *\n",
    "#from q3_sgd import *\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# We are going to train 10-dimensional vectors for this assignment\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size\n",
    "C = 5\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "\n",
    "startTime=time.time()\n",
    "wordVectors = np.concatenate(\n",
    "    ((np.random.rand(nWords, dimVectors) - 0.5) /\n",
    "       dimVectors, np.zeros((nWords, dimVectors))),\n",
    "    axis=0)\n",
    "wordVectors = sgd(\n",
    "    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C,\n",
    "        negSamplingCostAndGradient),\n",
    "    wordVectors, 0.3, 40000, None, True, PRINT_EVERY=10)\n",
    "# Note that normalization is not called here. This is not a bug,\n",
    "# normalizing during training loses the notion of length.\n",
    "\n",
    "print (\"sanity check: cost at convergence should be around or below 10\")\n",
    "print (\"training took %d seconds\" % (time.time() - startTime))\n",
    "\n",
    "# concatenate the input and output word vectors\n",
    "wordVectors = np.concatenate(\n",
    "    (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n",
    "    axis=0)\n",
    "# wordVectors = wordVectors[:nWords,:] + wordVectors[nWords:,:]\n",
    "\n",
    "visualizeWords = [\n",
    "    \"the\", \"a\", \"an\", \",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\",\n",
    "    \"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "    \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"waste\", \"dumb\",\n",
    "    \"annoying\"]\n",
    "\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U,S,V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:,0:2])\n",
    "\n",
    "for i in range(len(visualizeWords)):\n",
    "    plt.text(coord[i,0], coord[i,1], visualizeWords[i],\n",
    "        bbox=dict(facecolor='green', alpha=0.1))\n",
    "\n",
    "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n",
    "\n",
    "plt.savefig('q3_word_vectors.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Sentiment Analysis\n",
    "See details: http://web.stanford.edu/class/cs224n/assignment1/assignment1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "DEFAULT_FILE_PATH = \"/Users/KunWuYao/utils/datasets/glove.6B.50d.txt\"\n",
    "\n",
    "def loadWordVectors(tokens, filepath=DEFAULT_FILE_PATH, dimensions=50):\n",
    "    \"\"\"Read pretrained GloVe vectors\"\"\"\n",
    "    wordVectors = np.zeros((len(tokens), dimensions))\n",
    "    with open(filepath) as ifs:\n",
    "        for line in ifs:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            row = line.split()\n",
    "            token = row[0]\n",
    "            if token not in tokens:\n",
    "                continue\n",
    "            data = [float(x) for x in row[1:]]\n",
    "            if len(data) != dimensions:\n",
    "                raise RuntimeError(\"wrong number of dimensions\")\n",
    "            wordVectors[tokens[token]] = np.asarray(data)\n",
    "    return wordVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'but in imax 3-d , the clichÃ£Â©s disappear into the vertiginous perspectives opened up by the photography .'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-2524c4f4d574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-231-2524c4f4d574>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;31m# Load the train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTrainSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0mnTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mtrainFeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimVectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-228-05187bc8da7a>\u001b[0m in \u001b[0;36mgetTrainSentences\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetTrainSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetSplitSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetSplitSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-228-05187bc8da7a>\u001b[0m in \u001b[0;36mgetSplitSentences\u001b[0;34m(self, split)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetSplitSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mds_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msampleTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-228-05187bc8da7a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetSplitSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mds_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msampleTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-228-05187bc8da7a>\u001b[0m in \u001b[0;36msent_labels\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mfull_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-lrb-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'('\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-rrb-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m')'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0msent_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfull_sent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'but in imax 3-d , the clichÃ£Â©s disappear into the vertiginous perspectives opened up by the photography .'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "#from utils.treebank import StanfordSentiment\n",
    "#import utils.glove as glove\n",
    "\n",
    "#from q3_sgd import load_saved_params, sgd\n",
    "\n",
    "# We will use sklearn here because it will run faster than implementing\n",
    "# ourselves. However, for other parts of this assignment you must implement\n",
    "# the functions yourself!\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def getArguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    group = parser.add_mutually_exclusive_group(required=True)\n",
    "    group.add_argument(\"--pretrained\", dest=\"pretrained\", action=\"store_true\",\n",
    "                       help=\"Use pretrained GloVe vectors.\")\n",
    "    group.add_argument(\"--yourvectors\", dest=\"yourvectors\", action=\"store_true\",\n",
    "                       help=\"Use your vectors from q3.\")\n",
    "    args = parser.parse_args(['--pretrained'])\n",
    "    return args\n",
    "\n",
    "\n",
    "def getSentenceFeatures(tokens, wordVectors, sentence):\n",
    "    \"\"\"\n",
    "    Obtain the sentence feature for sentiment analysis by averaging its\n",
    "    word vectors\n",
    "    \"\"\"\n",
    "\n",
    "    # Implement computation for the sentence features given a sentence.\n",
    "\n",
    "    # Inputs:\n",
    "    # tokens -- a dictionary that maps words to their indices in\n",
    "    #           the word vector list\n",
    "    # wordVectors -- word vectors (each row) for all tokens\n",
    "    # sentence -- a list of words in the sentence of interest\n",
    "\n",
    "    # Output:\n",
    "    # - sentVector: feature vector for the sentence\n",
    "\n",
    "    sentVector = np.zeros((wordVectors.shape[1],))\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    for word in sentence:\n",
    "        sentVector +=wordVectors[tokens[word]]\n",
    "    sentVector/=len(sentence)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    assert sentVector.shape == (wordVectors.shape[1],)\n",
    "    return sentVector\n",
    "\n",
    "\n",
    "def getRegularizationValues():\n",
    "    \"\"\"Try different regularizations\n",
    "    Return a sorted list of values to try.\n",
    "    \"\"\"\n",
    "    values = None   # Assign a list of floats in the block below\n",
    "    ### YOUR CODE HERE\n",
    "    values=10**(np.random.uniform(-10,5,20))\n",
    "    ### END YOUR CODE\n",
    "    return sorted(values)\n",
    "\n",
    "\n",
    "def chooseBestModel(results):\n",
    "    \"\"\"Choose the best model based on parameter tuning on the dev set\n",
    "    Arguments:\n",
    "    results -- A list of python dictionaries of the following format:\n",
    "        {\n",
    "            \"reg\": regularization,\n",
    "            \"clf\": classifier,\n",
    "            \"train\": trainAccuracy,\n",
    "            \"dev\": devAccuracy,\n",
    "            \"test\": testAccuracy\n",
    "        }\n",
    "    Returns:\n",
    "    Your chosen result dictionary.\n",
    "    \"\"\"\n",
    "    bestResult = None\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    bestResult=max(results, key=lambda x: x[\"dev\"])\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return bestResult\n",
    "\n",
    "\n",
    "def accuracy(y, yhat):\n",
    "    \"\"\" Precision for classifier \"\"\"\n",
    "    assert(y.shape == yhat.shape)\n",
    "    return np.sum(y == yhat) * 100.0 / y.size\n",
    "\n",
    "\n",
    "def plotRegVsAccuracy(regValues, results, filename):\n",
    "    \"\"\" Make a plot of regularization vs accuracy \"\"\"\n",
    "    plt.plot(regValues, [x[\"train\"] for x in results])\n",
    "    plt.plot(regValues, [x[\"dev\"] for x in results])\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(\"regularization\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend(['train', 'dev'], loc='upper left')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "def outputConfusionMatrix(features, labels, clf, filename):\n",
    "    \"\"\" Generate a confusion matrix \"\"\"\n",
    "    pred = clf.predict(features)\n",
    "    cm = confusion_matrix(labels, pred, labels=range(5))\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Reds)\n",
    "    plt.colorbar()\n",
    "    classes = [\"- -\", \"-\", \"neut\", \"+\", \"+ +\"]\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "def outputPredictions(dataset, features, labels, clf, filename):\n",
    "    \"\"\" Write the predictions to file \"\"\"\n",
    "    pred = clf.predict(features)\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"True\\tPredicted\\tText\")\n",
    "        for i in range(len(dataset)):\n",
    "            f.write(\"%d\\t%d\\t%s\" % (\n",
    "                labels[i], pred[i], \" \".join(dataset[i][0])))\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \"\"\" Train a model to do sentiment analyis\"\"\"\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = StanfordSentiment()\n",
    "    tokens = dataset.tokens()\n",
    "    nWords = len(tokens)\n",
    "\n",
    "    if args.yourvectors:\n",
    "        _, wordVectors, _ = load_saved_params()\n",
    "        wordVectors = np.concatenate(\n",
    "            (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n",
    "            axis=1)\n",
    "    elif args.pretrained:\n",
    "        wordVectors = loadWordVectors(tokens)\n",
    "    dimVectors = wordVectors.shape[1]\n",
    "\n",
    "    # Load the train set\n",
    "    trainset = dataset.getTrainSentences()\n",
    "    nTrain = len(trainset)\n",
    "    trainFeatures = np.zeros((nTrain, dimVectors))\n",
    "    trainLabels = np.zeros((nTrain,), dtype=np.int32)\n",
    "    for i in range(nTrain):\n",
    "        words, trainLabels[i] = trainset[i]\n",
    "        trainFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n",
    "\n",
    "    # Prepare dev set features\n",
    "    devset = dataset.getDevSentences()\n",
    "    nDev = len(devset)\n",
    "    devFeatures = np.zeros((nDev, dimVectors))\n",
    "    devLabels = np.zeros((nDev,), dtype=np.int32)\n",
    "    for i in range(nDev):\n",
    "        words, devLabels[i] = devset[i]\n",
    "        devFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n",
    "\n",
    "    # Prepare test set features\n",
    "    testset = dataset.getTestSentences()\n",
    "    nTest = len(testset)\n",
    "    testFeatures = np.zeros((nTest, dimVectors))\n",
    "    testLabels = np.zeros((nTest,), dtype=np.int32)\n",
    "    for i in range(nTest):\n",
    "        words, testLabels[i] = testset[i]\n",
    "        testFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n",
    "\n",
    "    # We will save our results from each run\n",
    "    results = []\n",
    "    regValues = getRegularizationValues()\n",
    "    for reg in regValues:\n",
    "        print(\"Training for reg=%f\" % reg)\n",
    "        # Note: add a very small number to regularization to please the library\n",
    "        clf = LogisticRegression(C=1.0/(reg + 1e-12))\n",
    "        clf.fit(trainFeatures, trainLabels)\n",
    "\n",
    "        # Test on train set\n",
    "        pred = clf.predict(trainFeatures)\n",
    "        trainAccuracy = accuracy(trainLabels, pred)\n",
    "        print(\"Train accuracy (%%): %f\" % trainAccuracy)\n",
    "\n",
    "        # Test on dev set\n",
    "        pred = clf.predict(devFeatures)\n",
    "        devAccuracy = accuracy(devLabels, pred)\n",
    "        print(\"Dev accuracy (%%): %f\" % devAccuracy)\n",
    "\n",
    "        # Test on test set\n",
    "        # Note: always running on test is poor style. Typically, you should\n",
    "        # do this only after validation.\n",
    "        pred = clf.predict(testFeatures)\n",
    "        testAccuracy = accuracy(testLabels, pred)\n",
    "        print(\"Test accuracy (%%): %f\" % testAccuracy)\n",
    "\n",
    "        results.append({\n",
    "            \"reg\": reg,\n",
    "            \"clf\": clf,\n",
    "            \"train\": trainAccuracy,\n",
    "            \"dev\": devAccuracy,\n",
    "            \"test\": testAccuracy})\n",
    "\n",
    "    # Print the accuracies\n",
    "    print(\"\")\n",
    "    print(\"=== Recap ===\")\n",
    "    print(\"Reg\\t\\tTrain\\tDev\\tTest\")\n",
    "    for result in results:\n",
    "        print(\"%.2E\\t%.3f\\t%.3f\\t%.3f\" % (\n",
    "            result[\"reg\"],\n",
    "            result[\"train\"],\n",
    "            result[\"dev\"],\n",
    "            result[\"test\"]))\n",
    "    print(\"\")\n",
    "\n",
    "    bestResult = chooseBestModel(results)\n",
    "    print(\"Best regularization value: %0.2E\" % bestResult[\"reg\"])\n",
    "    print(\"Test accuracy (%%): %f\" % bestResult[\"test\"])\n",
    "\n",
    "    # do some error analysis\n",
    "    if args.pretrained:\n",
    "        plotRegVsAccuracy(regValues, results, \"q4_reg_v_acc.png\")\n",
    "        outputConfusionMatrix(devFeatures, devLabels, bestResult[\"clf\"],\n",
    "                              \"q4_dev_conf.png\")\n",
    "        outputPredictions(devset, devFeatures, devLabels, bestResult[\"clf\"],\n",
    "                          \"q4_dev_pred.txt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(getArguments())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clichÃ©s'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'clichÃ©s'.encode().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clichÃ©s\n"
     ]
    }
   ],
   "source": [
    "with open('test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
